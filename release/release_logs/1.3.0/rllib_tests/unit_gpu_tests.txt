Script started on 2021-04-09 04:56:02-0700
Collecting package metadata (repodata.json): - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / done
Solving environment: \ failed

PackagesNotFoundError: The following packages are missing from the target environment:
  - terminado


Requirement already satisfied: pip in /home/ray/anaconda3/lib/python3.7/site-packages (20.3.3)
Collecting pip
  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)
[?25l
[K     |‚ñè                               | 10 kB 27.6 MB/s eta 0:00:01
[K     |‚ñç                               | 20 kB 21.7 MB/s eta 0:00:01
[K     |‚ñã                               | 30 kB 11.7 MB/s eta 0:00:01
[K     |‚ñâ                               | 40 kB 9.5 MB/s eta 0:00:01
[K     |‚ñà                               | 51 kB 6.7 MB/s eta 0:00:01
[K     |‚ñà‚ñé                              | 61 kB 7.2 MB/s eta 0:00:01
[K     |‚ñà‚ñå                              | 71 kB 7.7 MB/s eta 0:00:01
[K     |‚ñà‚ñä                              | 81 kB 8.1 MB/s eta 0:00:01
[K     |‚ñà‚ñà                              | 92 kB 7.7 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñè                             | 102 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñç                             | 112 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñå                             | 122 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñä                             | 133 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà                             | 143 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñè                            | 153 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñç                            | 163 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñã                            | 174 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñâ                            | 184 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà                            | 194 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñé                           | 204 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñå                           | 215 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñä                           | 225 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà                           | 235 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà                           | 245 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 256 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 266 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 276 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 286 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 296 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 307 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 317 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 327 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 337 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 348 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 358 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 368 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 378 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 389 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 399 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 409 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 419 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 430 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 440 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 450 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 460 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 471 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 481 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 491 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 501 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 512 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 522 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 532 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 542 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 552 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 563 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 573 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 583 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 593 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 604 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 614 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 624 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 634 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 645 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 655 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 665 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 675 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 686 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 696 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 706 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 716 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 727 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 737 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 747 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 757 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 768 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 778 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 788 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 798 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 808 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 819 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 829 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 839 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 849 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 860 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 870 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 880 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 890 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 901 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 911 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 921 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 931 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 942 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 952 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 962 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 972 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 983 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 993 kB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 1.0 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 1.0 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 1.0 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 1.0 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 1.0 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 1.1 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 1.2 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1.3 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1.4 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5 MB 7.0 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5 MB 7.0 MB/s 
[?25hInstalling collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.3.3
    Uninstalling pip-20.3.3:
      Successfully uninstalled pip-20.3.3
Successfully installed pip-21.0.1
Requirement already satisfied: pytest in /home/ray/anaconda3/lib/python3.7/site-packages (5.4.3)
Collecting pytest
  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)
[?25l
[K     |‚ñà‚ñè                              | 10 kB 22.9 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñç                             | 20 kB 19.2 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñå                            | 30 kB 10.6 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñä                           | 40 kB 8.7 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 51 kB 6.4 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 61 kB 6.2 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 71 kB 6.6 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 81 kB 6.6 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 92 kB 7.1 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 102 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 112 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 122 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 133 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 143 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 153 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 163 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 174 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 184 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 194 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 204 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 215 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 225 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 235 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 245 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 256 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 266 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 276 kB 5.5 MB/s eta 0:00:01
[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 280 kB 5.5 MB/s 
[?25hRequirement already satisfied: attrs>=19.2.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from pytest) (20.3.0)
Collecting iniconfig
  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)
Collecting toml
  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)
Requirement already satisfied: importlib-metadata>=0.12 in /home/ray/anaconda3/lib/python3.7/site-packages (from pytest) (3.4.0)
Requirement already satisfied: packaging in /home/ray/anaconda3/lib/python3.7/site-packages (from pytest) (20.8)
Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /home/ray/anaconda3/lib/python3.7/site-packages (from pytest) (0.13.1)
Requirement already satisfied: py>=1.8.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from pytest) (1.10.0)
Requirement already satisfied: zipp>=0.5 in /home/ray/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.4.0)
Requirement already satisfied: typing-extensions>=3.6.4 in /home/ray/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.7.4.3)
Requirement already satisfied: pyparsing>=2.0.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from packaging->pytest) (2.4.7)
Installing collected packages: toml, iniconfig, pytest
  Attempting uninstall: pytest
    Found existing installation: pytest 5.4.3
    Uninstalling pytest-5.4.3:
      Successfully uninstalled pytest-5.4.3
Successfully installed iniconfig-1.1.1 pytest-6.2.3 toml-0.10.2
Requirement already satisfied: terminado in /home/ray/anaconda3/lib/python3.7/site-packages (0.9.2)
Requirement already satisfied: tornado>=4 in /home/ray/anaconda3/lib/python3.7/site-packages (from terminado) (6.1)
Requirement already satisfied: ptyprocess in /home/ray/anaconda3/lib/python3.7/site-packages (from terminado) (0.7.0)
Requirement already satisfied: tensorflow-gpu in /home/ray/anaconda3/lib/python3.7/site-packages (2.4.1)
Requirement already satisfied: wheel~=0.35 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.36.2)
Requirement already satisfied: protobuf>=3.9.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (3.15.8)
Requirement already satisfied: tensorboard~=2.4 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.4.1)
Requirement already satisfied: h5py~=2.10.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.10.0)
Requirement already satisfied: google-pasta~=0.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.2.0)
Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (2.4.0)
Requirement already satisfied: flatbuffers~=1.12.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.12)
Requirement already satisfied: grpcio~=1.32.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.32.0)
Requirement already satisfied: typing-extensions~=3.7.4 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (3.7.4.3)
Requirement already satisfied: wrapt~=1.12.1 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.12.1)
Requirement already satisfied: opt-einsum~=3.3.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (3.3.0)
Requirement already satisfied: astunparse~=1.6.3 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.6.3)
Requirement already satisfied: gast==0.3.3 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.3.3)
Requirement already satisfied: absl-py~=0.10 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (0.11.0)
Requirement already satisfied: six~=1.15.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.15.0)
Requirement already satisfied: numpy~=1.19.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.19.5)
Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.2)
Requirement already satisfied: termcolor~=1.1.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)
Requirement already satisfied: requests<3,>=2.21.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (2.25.1)
Requirement already satisfied: setuptools>=41.0.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (52.0.0.post20210125)
Requirement already satisfied: werkzeug>=0.11.15 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)
Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.24.0)
Requirement already satisfied: markdown>=2.6.8 in /home/ray/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.0)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ray/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)
Requirement already satisfied: rsa<5,>=3.1.4 in /home/ray/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)
Requirement already satisfied: importlib-metadata in /home/ray/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ray/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ray/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.26.2)
Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)
Requirement already satisfied: chardet<5,>=3.0.2 in /home/ray/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)
Requirement already satisfied: idna<3,>=2.5 in /home/ray/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)
Requirement already satisfied: oauthlib>=3.0.0 in /home/ray/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)
Requirement already satisfied: zipp>=0.5 in /home/ray/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)
Cloning into 'ray'...
remote: Enumerating objects: 152, done.[K
remote: Counting objects:   0% (1/152)[K
remote: Counting objects:   1% (2/152)[K
remote: Counting objects:   2% (4/152)[K
remote: Counting objects:   3% (5/152)[K
remote: Counting objects:   4% (7/152)[K
remote: Counting objects:   5% (8/152)[K
remote: Counting objects:   6% (10/152)[K
remote: Counting objects:   7% (11/152)[K
remote: Counting objects:   8% (13/152)[K
remote: Counting objects:   9% (14/152)[K
remote: Counting objects:  10% (16/152)[K
remote: Counting objects:  11% (17/152)[K
remote: Counting objects:  12% (19/152)[K
remote: Counting objects:  13% (20/152)[K
remote: Counting objects:  14% (22/152)[K
remote: Counting objects:  15% (23/152)[K
remote: Counting objects:  16% (25/152)[K
remote: Counting objects:  17% (26/152)[K
remote: Counting objects:  18% (28/152)[K
remote: Counting objects:  19% (29/152)[K
remote: Counting objects:  20% (31/152)[K
remote: Counting objects:  21% (32/152)[K
remote: Counting objects:  22% (34/152)[K
remote: Counting objects:  23% (35/152)[K
remote: Counting objects:  24% (37/152)[K
remote: Counting objects:  25% (38/152)[K
remote: Counting objects:  26% (40/152)[K
remote: Counting objects:  27% (42/152)[K
remote: Counting objects:  28% (43/152)[K
remote: Counting objects:  29% (45/152)[K
remote: Counting objects:  30% (46/152)[K
remote: Counting objects:  31% (48/152)[K
remote: Counting objects:  32% (49/152)[K
remote: Counting objects:  33% (51/152)[K
remote: Counting objects:  34% (52/152)[K
remote: Counting objects:  35% (54/152)[K
remote: Counting objects:  36% (55/152)[K
remote: Counting objects:  37% (57/152)[K
remote: Counting objects:  38% (58/152)[K
remote: Counting objects:  39% (60/152)[K
remote: Counting objects:  40% (61/152)[K
remote: Counting objects:  41% (63/152)[K
remote: Counting objects:  42% (64/152)[K
remote: Counting objects:  43% (66/152)[K
remote: Counting objects:  44% (67/152)[K
remote: Counting objects:  45% (69/152)[K
remote: Counting objects:  46% (70/152)[K
remote: Counting objects:  47% (72/152)[K
remote: Counting objects:  48% (73/152)[K
remote: Counting objects:  49% (75/152)[K
remote: Counting objects:  50% (76/152)[K
remote: Counting objects:  51% (78/152)[K
remote: Counting objects:  52% (80/152)[K
remote: Counting objects:  53% (81/152)[K
remote: Counting objects:  54% (83/152)[K
remote: Counting objects:  55% (84/152)[K
remote: Counting objects:  56% (86/152)[K
remote: Counting objects:  57% (87/152)[K
remote: Counting objects:  58% (89/152)[K
remote: Counting objects:  59% (90/152)[K
remote: Counting objects:  60% (92/152)[K
remote: Counting objects:  61% (93/152)[K
remote: Counting objects:  62% (95/152)[K
remote: Counting objects:  63% (96/152)[K
remote: Counting objects:  64% (98/152)[K
remote: Counting objects:  65% (99/152)[K
remote: Counting objects:  66% (101/152)[K
remote: Counting objects:  67% (102/152)[K
remote: Counting objects:  68% (104/152)[K
remote: Counting objects:  69% (105/152)[K
remote: Counting objects:  70% (107/152)[K
remote: Counting objects:  71% (108/152)[K
remote: Counting objects:  72% (110/152)[K
remote: Counting objects:  73% (111/152)[K
remote: Counting objects:  74% (113/152)[K
remote: Counting objects:  75% (114/152)[K
remote: Counting objects:  76% (116/152)[K
remote: Counting objects:  77% (118/152)[K
remote: Counting objects:  78% (119/152)[K
remote: Counting objects:  79% (121/152)[K
remote: Counting objects:  80% (122/152)[K
remote: Counting objects:  81% (124/152)[K
remote: Counting objects:  82% (125/152)[K
remote: Counting objects:  83% (127/152)[K
remote: Counting objects:  84% (128/152)[K
remote: Counting objects:  85% (130/152)[K
remote: Counting objects:  86% (131/152)[K
remote: Counting objects:  87% (133/152)[K
remote: Counting objects:  88% (134/152)[K
remote: Counting objects:  89% (136/152)[K
remote: Counting objects:  90% (137/152)[K
remote: Counting objects:  91% (139/152)[K
remote: Counting objects:  92% (140/152)[K
remote: Counting objects:  93% (142/152)[K
remote: Counting objects:  94% (143/152)[K
remote: Counting objects:  95% (145/152)[K
remote: Counting objects:  96% (146/152)[K
remote: Counting objects:  97% (148/152)[K
remote: Counting objects:  98% (149/152)[K
remote: Counting objects:  99% (151/152)[K
remote: Counting objects: 100% (152/152)[K
remote: Counting objects: 100% (152/152), done.[K
remote: Compressing objects:   0% (1/117)[K
remote: Compressing objects:   1% (2/117)[K
remote: Compressing objects:   2% (3/117)[K
remote: Compressing objects:   3% (4/117)[K
remote: Compressing objects:   4% (5/117)[K
remote: Compressing objects:   5% (6/117)[K
remote: Compressing objects:   6% (8/117)[K
remote: Compressing objects:   7% (9/117)[K
remote: Compressing objects:   8% (10/117)[K
remote: Compressing objects:   9% (11/117)[K
remote: Compressing objects:  10% (12/117)[K
remote: Compressing objects:  11% (13/117)[K
remote: Compressing objects:  12% (15/117)[K
remote: Compressing objects:  13% (16/117)[K
remote: Compressing objects:  14% (17/117)[K
remote: Compressing objects:  15% (18/117)[K
remote: Compressing objects:  16% (19/117)[K
remote: Compressing objects:  17% (20/117)[K
remote: Compressing objects:  18% (22/117)[K
remote: Compressing objects:  19% (23/117)[K
remote: Compressing objects:  20% (24/117)[K
remote: Compressing objects:  21% (25/117)[K
remote: Compressing objects:  22% (26/117)[K
remote: Compressing objects:  23% (27/117)[K
remote: Compressing objects:  24% (29/117)[K
remote: Compressing objects:  25% (30/117)[K
remote: Compressing objects:  26% (31/117)[K
remote: Compressing objects:  27% (32/117)[K
remote: Compressing objects:  28% (33/117)[K
remote: Compressing objects:  29% (34/117)[K
remote: Compressing objects:  30% (36/117)[K
remote: Compressing objects:  31% (37/117)[K
remote: Compressing objects:  32% (38/117)[K
remote: Compressing objects:  33% (39/117)[K
remote: Compressing objects:  34% (40/117)[K
remote: Compressing objects:  35% (41/117)[K
remote: Compressing objects:  36% (43/117)[K
remote: Compressing objects:  37% (44/117)[K
remote: Compressing objects:  38% (45/117)[K
remote: Compressing objects:  39% (46/117)[K
remote: Compressing objects:  40% (47/117)[K
remote: Compressing objects:  41% (48/117)[K
remote: Compressing objects:  42% (50/117)[K
remote: Compressing objects:  43% (51/117)[K
remote: Compressing objects:  44% (52/117)[K
remote: Compressing objects:  45% (53/117)[K
remote: Compressing objects:  46% (54/117)[K
remote: Compressing objects:  47% (55/117)[K
remote: Compressing objects:  48% (57/117)[K
remote: Compressing objects:  49% (58/117)[K
remote: Compressing objects:  50% (59/117)[K
remote: Compressing objects:  51% (60/117)[K
remote: Compressing objects:  52% (61/117)[K
remote: Compressing objects:  53% (63/117)[K
remote: Compressing objects:  54% (64/117)[K
remote: Compressing objects:  55% (65/117)[K
remote: Compressing objects:  56% (66/117)[K
remote: Compressing objects:  57% (67/117)[K
remote: Compressing objects:  58% (68/117)[K
remote: Compressing objects:  59% (70/117)[K
remote: Compressing objects:  60% (71/117)[K
remote: Compressing objects:  61% (72/117)[K
remote: Compressing objects:  62% (73/117)[K
remote: Compressing objects:  63% (74/117)[K
remote: Compressing objects:  64% (75/117)[K
remote: Compressing objects:  65% (77/117)[K
remote: Compressing objects:  66% (78/117)[K
remote: Compressing objects:  67% (79/117)[K
remote: Compressing objects:  68% (80/117)[K
remote: Compressing objects:  69% (81/117)[K
remote: Compressing objects:  70% (82/117)[K
remote: Compressing objects:  71% (84/117)[K
remote: Compressing objects:  72% (85/117)[K
remote: Compressing objects:  73% (86/117)[K
remote: Compressing objects:  74% (87/117)[K
remote: Compressing objects:  75% (88/117)[K
remote: Compressing objects:  76% (89/117)[K
remote: Compressing objects:  77% (91/117)[K
remote: Compressing objects:  78% (92/117)[K
remote: Compressing objects:  79% (93/117)[K
remote: Compressing objects:  80% (94/117)[K
remote: Compressing objects:  81% (95/117)[K
remote: Compressing objects:  82% (96/117)[K
remote: Compressing objects:  83% (98/117)[K
remote: Compressing objects:  84% (99/117)[K
remote: Compressing objects:  85% (100/117)[K
remote: Compressing objects:  86% (101/117)[K
remote: Compressing objects:  87% (102/117)[K
remote: Compressing objects:  88% (103/117)[K
remote: Compressing objects:  89% (105/117)[K
remote: Compressing objects:  90% (106/117)[K
remote: Compressing objects:  91% (107/117)[K
remote: Compressing objects:  92% (108/117)[K
remote: Compressing objects:  93% (109/117)[K
remote: Compressing objects:  94% (110/117)[K
remote: Compressing objects:  95% (112/117)[K
remote: Compressing objects:  96% (113/117)[K
remote: Compressing objects:  97% (114/117)[K
remote: Compressing objects:  98% (115/117)[K
remote: Compressing objects:  99% (116/117)[K
remote: Compressing objects: 100% (117/117)[K
remote: Compressing objects: 100% (117/117), done.[K
Receiving objects:   0% (1/118445)   
Receiving objects:   1% (1185/118445)   
Receiving objects:   2% (2369/118445)   
Receiving objects:   3% (3554/118445)   
Receiving objects:   4% (4738/118445)   
Receiving objects:   5% (5923/118445)   
Receiving objects:   6% (7107/118445)   
Receiving objects:   7% (8292/118445)   
Receiving objects:   8% (9476/118445)   
Receiving objects:   9% (10661/118445)   
Receiving objects:  10% (11845/118445)   
Receiving objects:  11% (13029/118445)   
Receiving objects:  12% (14214/118445)   
Receiving objects:  13% (15398/118445), 15.16 MiB | 30.32 MiB/s   
Receiving objects:  14% (16583/118445), 15.16 MiB | 30.32 MiB/s   
Receiving objects:  15% (17767/118445), 15.16 MiB | 30.32 MiB/s   
Receiving objects:  16% (18952/118445), 15.16 MiB | 30.32 MiB/s   
Receiving objects:  17% (20136/118445), 15.16 MiB | 30.32 MiB/s   
Receiving objects:  17% (20369/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  18% (21321/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  19% (22505/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  20% (23689/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  21% (24874/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  22% (26058/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  23% (27243/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  24% (28427/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  25% (29612/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  26% (30796/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  27% (31981/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  28% (33165/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  29% (34350/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  30% (35534/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  31% (36718/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  32% (37903/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  33% (39087/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  34% (40272/118445), 29.73 MiB | 29.73 MiB/s   
Receiving objects:  35% (41456/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  36% (42641/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  37% (43825/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  38% (45010/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  39% (46194/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  40% (47378/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  41% (48563/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  42% (49747/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  43% (50932/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  44% (52116/118445), 47.22 MiB | 31.48 MiB/s   
Receiving objects:  44% (52205/118445), 61.04 MiB | 30.52 MiB/s   
Receiving objects:  45% (53301/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  46% (54485/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  47% (55670/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  48% (56854/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  49% (58039/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  50% (59223/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  51% (60407/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  52% (61592/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  53% (62776/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  54% (63961/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  55% (65145/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  56% (66330/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  56% (67188/118445), 71.18 MiB | 28.47 MiB/s   
Receiving objects:  57% (67514/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  58% (68699/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  59% (69883/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  60% (71067/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  61% (72252/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  62% (73436/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  63% (74621/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  64% (75805/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  65% (76990/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  66% (78174/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  67% (79359/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  68% (80543/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  69% (81728/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  70% (82912/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  71% (84096/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  72% (85281/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  73% (86465/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  74% (87650/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  75% (88834/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  76% (90019/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  77% (91203/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  78% (92388/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  79% (93572/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  80% (94756/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  81% (95941/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  82% (97125/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  83% (98310/118445), 86.65 MiB | 28.88 MiB/s   
Receiving objects:  84% (99494/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  85% (100679/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  86% (101863/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  87% (103048/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  88% (104232/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  89% (105417/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  90% (106601/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  91% (107785/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  92% (108970/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  93% (110154/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  94% (111339/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  95% (112523/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  96% (113708/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  97% (114892/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  98% (116077/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects:  99% (117261/118445), 103.05 MiB | 29.44 MiB/s   
remote: Total 118445 (delta 65), reused 56 (delta 35), pack-reused 118293[K
Receiving objects: 100% (118445/118445), 103.05 MiB | 29.44 MiB/s   
Receiving objects: 100% (118445/118445), 111.92 MiB | 29.71 MiB/s, done.
Resolving deltas:   0% (0/86645)   
Resolving deltas:   1% (867/86645)   
Resolving deltas:   2% (1754/86645)   
Resolving deltas:   3% (2601/86645)   
Resolving deltas:   4% (3468/86645)   
Resolving deltas:   5% (4352/86645)   
Resolving deltas:   6% (5202/86645)   
Resolving deltas:   7% (6071/86645)   
Resolving deltas:   8% (6935/86645)   
Resolving deltas:   9% (7821/86645)   
Resolving deltas:  10% (8666/86645)   
Resolving deltas:  11% (9554/86645)   
Resolving deltas:  12% (10417/86645)   
Resolving deltas:  13% (11278/86645)   
Resolving deltas:  14% (12133/86645)   
Resolving deltas:  15% (12999/86645)   
Resolving deltas:  16% (13868/86645)   
Resolving deltas:  17% (14732/86645)   
Resolving deltas:  18% (15768/86645)   
Resolving deltas:  18% (16144/86645)   
Resolving deltas:  19% (16476/86645)   
Resolving deltas:  20% (17340/86645)   
Resolving deltas:  21% (18205/86645)   
Resolving deltas:  22% (19062/86645)   
Resolving deltas:  23% (19937/86645)   
Resolving deltas:  24% (20796/86645)   
Resolving deltas:  25% (21740/86645)   
Resolving deltas:  26% (22662/86645)   
Resolving deltas:  27% (23419/86645)   
Resolving deltas:  28% (24270/86645)   
Resolving deltas:  29% (25148/86645)   
Resolving deltas:  30% (26003/86645)   
Resolving deltas:  31% (26863/86645)   
Resolving deltas:  32% (27733/86645)   
Resolving deltas:  33% (28622/86645)   
Resolving deltas:  34% (29461/86645)   
Resolving deltas:  35% (30341/86645)   
Resolving deltas:  36% (31211/86645)   
Resolving deltas:  37% (32064/86645)   
Resolving deltas:  38% (32926/86645)   
Resolving deltas:  39% (33795/86645)   
Resolving deltas:  40% (34658/86645)   
Resolving deltas:  41% (35564/86645)   
Resolving deltas:  42% (36393/86645)   
Resolving deltas:  42% (36419/86645)   
Resolving deltas:  43% (37915/86645)   
Resolving deltas:  44% (38303/86645)   
Resolving deltas:  45% (39007/86645)   
Resolving deltas:  46% (39948/86645)   
Resolving deltas:  47% (40725/86645)   
Resolving deltas:  48% (41671/86645)   
Resolving deltas:  49% (42459/86645)   
Resolving deltas:  50% (43334/86645)   
Resolving deltas:  51% (44191/86645)   
Resolving deltas:  52% (45094/86645)   
Resolving deltas:  53% (45944/86645)   
Resolving deltas:  54% (46865/86645)   
Resolving deltas:  55% (47657/86645)   
Resolving deltas:  56% (48522/86645)   
Resolving deltas:  57% (49401/86645)   
Resolving deltas:  58% (50259/86645)   
Resolving deltas:  59% (51126/86645)   
Resolving deltas:  60% (51991/86645)   
Resolving deltas:  61% (52872/86645)   
Resolving deltas:  62% (53722/86645)   
Resolving deltas:  63% (54720/86645)   
Resolving deltas:  64% (55461/86645)   
Resolving deltas:  65% (56323/86645)   
Resolving deltas:  66% (57205/86645)   
Resolving deltas:  67% (58072/86645)   
Resolving deltas:  68% (58920/86645)   
Resolving deltas:  69% (59786/86645)   
Resolving deltas:  70% (60659/86645)   
Resolving deltas:  71% (61520/86645)   
Resolving deltas:  72% (62404/86645)   
Resolving deltas:  73% (63256/86645)   
Resolving deltas:  74% (64126/86645)   
Resolving deltas:  75% (65011/86645)   
Resolving deltas:  76% (65853/86645)   
Resolving deltas:  77% (66724/86645)   
Resolving deltas:  78% (67585/86645)   
Resolving deltas:  79% (68482/86645)   
Resolving deltas:  80% (69317/86645)   
Resolving deltas:  81% (70226/86645)   
Resolving deltas:  82% (71049/86645)   
Resolving deltas:  83% (71919/86645)   
Resolving deltas:  83% (72755/86645)   
Resolving deltas:  84% (72782/86645)   
Resolving deltas:  85% (73650/86645)   
Resolving deltas:  86% (74517/86645)   
Resolving deltas:  87% (75430/86645)   
Resolving deltas:  88% (76255/86645)   
Resolving deltas:  89% (77155/86645)   
Resolving deltas:  90% (78033/86645)   
Resolving deltas:  91% (78854/86645)   
Resolving deltas:  92% (79719/86645)   
Resolving deltas:  93% (80597/86645)   
Resolving deltas:  94% (81460/86645)   
Resolving deltas:  95% (82343/86645)   
Resolving deltas:  96% (83181/86645)   
Resolving deltas:  97% (84047/86645)   
Resolving deltas:  98% (84919/86645)   
Resolving deltas:  99% (85779/86645)   
Resolving deltas: 100% (86645/86645)   
Resolving deltas: 100% (86645/86645), done.
~/release-automation-rllib_unit_gpu_tests/ray ~/release-automation-rllib_unit_gpu_tests
Note: checking out 'cb3661e547662f309a0cc55c5495b3adb779a309'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b <new-branch-name>

HEAD is now at cb3661e54 [dask-on-ray] Fix Dask-on-Ray scheduler break caused by changing internal Dask API. (#15100)
+ set -euo pipefail
+++ dirname ./ci/travis/install-bazel.sh
++ cd ./ci/travis
++ pwd
+ ROOT_DIR=/home/ray/release-automation-rllib_unit_gpu_tests/ray/ci/travis
+ arg1=
+ achitecture=x86_64
+ platform=unknown
+ case "${OSTYPE}" in
+ echo 'Platform is Linux (or WSL).'
Platform is Linux (or WSL).
+ platform=linux
+ git ls-files -s
+ set +x
+ export PATH=/opt/python/cp36-cp36m/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/condabin:/home/ray/anaconda3/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ PATH=/opt/python/cp36-cp36m/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/bin:/home/ray/anaconda3/condabin:/home/ray/anaconda3/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ command -v python3
+ python=/home/ray/anaconda3/bin/python3
++ /home/ray/anaconda3/bin/python3 -s -c 'import runpy, sys; runpy.run_path(sys.argv.pop(), run_name='\''__api__'\'')' bazel_version /home/ray/release-automation-rllib_unit_gpu_tests/ray/ci/travis/../../python/setup.py
+ version=3.2.0
+ '[' linux-gnu = msys ']'
+ target=./install.sh
+ curl -f -s -L -R -o ./install.sh https://github.com/bazelbuild/bazel/releases/download/3.2.0/bazel-3.2.0-installer-linux-x86_64.sh
+ chmod +x ./install.sh
+ '[' '' = true ']'
+ '[' '' = --system ']'
+ ./install.sh --user
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
+ rm -f ./install.sh
+ '[' '' = true ']'
+ '[' '' = true ']'
+ '[' '' = true ']'
(05:01:59) [32mINFO: [0mCurrent date is 2021-04-09
(05:01:59) [32mLoading:[0m 
(05:01:59) [32mLoading:[0m 0 packages loaded
(05:02:05) [33mDEBUG: [0m/home/ray/release-automation-rllib_unit_gpu_tests/ray/bazel/ray_deps_setup.bzl:63:14: No implicit mirrors used because urls were explicitly provided
(05:02:06) [32mAnalyzing:[0m 10 targets (1 packages loaded, 0 targets configured)
(05:02:23) [32mAnalyzing:[0m 10 targets (19 packages loaded, 107 targets configured)
(05:02:31) [32mINFO: [0mAnalyzed 10 targets (27 packages loaded, 438 targets configured).
(05:02:31) [32mINFO: [0mFound 10 test targets...
(05:02:31) [32m[0 / 4][0m [Prepa] BazelWorkspaceStatusAction stable-status.txt
(05:02:59) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 27s local ... (2 actions running)
(05:03:29) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 57s local ... (2 actions running)
(05:03:59) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 87s local ... (2 actions running)
(05:04:17) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 105s local ... (2 actions running)
(05:04:38) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 126s local ... (2 actions running)
(05:05:26) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 174s local ... (2 actions running)
(05:06:28) [32m[8 / 10][0m Testing //rllib:examples/attention_net_tf; 236s local ... (2 actions running)
(05:07:08) [32m[12 / 14][0m 1 / 10 tests;[0m Testing //rllib:examples/attention_net_tf; 277s local ... (2 actions running)
(05:07:55) [32m[12 / 14][0m 1 / 10 tests;[0m Testing //rllib:examples/batch_norm_model_dqn_tf; 84s local ... (2 actions running)
(05:08:48) [32m[16 / 18][0m 2 / 10 tests;[0m Testing //rllib:examples/attention_net_tf; 61s local ... (2 actions running)
(05:09:50) [32m[20 / 22][0m 3 / 10 tests;[0m Testing //rllib:examples/attention_net_tf; 123s local ... (2 actions running)
(05:11:01) [32m[20 / 22][0m 3 / 10 tests;[0m Testing //rllib:examples/attention_net_tf; 194s local ... (2 actions running)
(05:12:22) [32m[20 / 22][0m 3 / 10 tests;[0m Testing //rllib:examples/attention_net_tf; 275s local ... (2 actions running)

[31m[1mTIMEOUT: [0m//rllib:examples/batch_norm_model_ddpg_tf (Summary)
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/batch_norm_model_ddpg_tf/test.log
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/batch_norm_model_ddpg_tf/test_attempts/attempt_1.log
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/batch_norm_model_ddpg_tf/test_attempts/attempt_2.log
(05:12:56) [32mINFO: [0mFrom Testing //rllib:examples/batch_norm_model_ddpg_tf:
==================== Test output for //rllib:examples/batch_norm_model_ddpg_tf:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:09:27,616	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:09:33,482	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8267[39m[22m
[2m[36m(pid=3607)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=3607)[0m Instructions for updating:
[2m[36m(pid=3607)[0m non-resource variables are not supported in the long term
[2m[36m(pid=3607)[0m 2021-04-09 12:09:52,175	INFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=3607)[0m 2021-04-09 12:09:52,175	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=3607)[0m 2021-04-09 12:09:52,176	WARNING ddpg.py:158 -- Setting use_state_preprocessor=True since a custom model was specified.
[2m[36m(pid=3607)[0m 2021-04-09 12:09:56,857	WARNING ddpg_tf_policy.py:221 -- You are using a state-preprocessor with DDPG and therefore, `custom_loss` will be called on your Model! Please be aware that DDPG now uses the ModelV2 API, which merges all previously separate sub-models (policy_model, q_model, and twin_q_model) into one ModelV2, on which `custom_loss` is called, passing it [actor_loss, critic_loss] as 1st argument. You may have to change your custom loss function to handle this.
[2m[36m(pid=3607)[0m 2021-04-09 12:10:00,668	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=3607)[0m 2021-04-09 12:10:10,088	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
2021-04-09 12:10:10,768	INFO tune.py:549 -- Total run time: 33.62 seconds (32.80 seconds for the tuning loop).
-- Test timed out at 2021-04-09 12:10:11 UTC --
================================================================================
==================== Test output for //rllib:examples/batch_norm_model_ddpg_tf:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:10:41,526	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:10:46,661	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8267[39m[22m
[2m[36m(pid=3867)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=3867)[0m Instructions for updating:
[2m[36m(pid=3867)[0m non-resource variables are not supported in the long term
[2m[36m(pid=3867)[0m 2021-04-09 12:11:05,267	INFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=3867)[0m 2021-04-09 12:11:05,268	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=3867)[0m 2021-04-09 12:11:05,272	WARNING ddpg.py:158 -- Setting use_state_preprocessor=True since a custom model was specified.
[2m[36m(pid=3867)[0m 2021-04-09 12:11:11,329	WARNING ddpg_tf_policy.py:221 -- You are using a state-preprocessor with DDPG and therefore, `custom_loss` will be called on your Model! Please be aware that DDPG now uses the ModelV2 API, which merges all previously separate sub-models (policy_model, q_model, and twin_q_model) into one ModelV2, on which `custom_loss` is called, passing it [actor_loss, critic_loss] as 1st argument. You may have to change your custom loss function to handle this.
[2m[36m(pid=3867)[0m 2021-04-09 12:11:15,237	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=3867)[0m 2021-04-09 12:11:23,982	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
2021-04-09 12:11:24,777	INFO tune.py:549 -- Total run time: 35.14 seconds (33.71 seconds for the tuning loop).
-- Test timed out at 2021-04-09 12:11:26 UTC --
================================================================================
==================== Test output for //rllib:examples/batch_norm_model_ddpg_tf:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:11:56,327	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:12:02,490	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8267[39m[22m
[2m[36m(pid=4130)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=4130)[0m Instructions for updating:
[2m[36m(pid=4130)[0m non-resource variables are not supported in the long term
[2m[36m(pid=4130)[0m 2021-04-09 12:12:22,482	INFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=4130)[0m 2021-04-09 12:12:22,482	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=4130)[0m 2021-04-09 12:12:22,483	WARNING ddpg.py:158 -- Setting use_state_preprocessor=True since a custom model was specified.
[2m[36m(pid=4130)[0m 2021-04-09 12:12:26,787	WARNING ddpg_tf_policy.py:221 -- You are using a state-preprocessor with DDPG and therefore, `custom_loss` will be called on your Model! Please be aware that DDPG now uses the ModelV2 API, which merges all previously separate sub-models (policy_model, q_model, and twin_q_model) into one ModelV2, on which `custom_loss` is called, passing it [actor_loss, critic_loss] as 1st argument. You may have to change your custom loss function to handle this.
[2m[36m(pid=4130)[0m 2021-04-09 12:12:30,921	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=4130)[0m 2021-04-09 12:12:40,548	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
2021-04-09 12:12:41,050	INFO tune.py:549 -- Total run time: 34.02 seconds (33.39 seconds for the tuning loop).
-- Test timed out at 2021-04-09 12:12:41 UTC --
================================================================================
(05:14:38) [32m[24 / 26][0m 4 / 10 tests, [31m[1m1 failed[0m;[0m Testing //rllib:examples/batch_norm_model_dqn_torch; 101s local ... (2 actions running)
(05:17:23) [32m[24 / 26][0m 4 / 10 tests, [31m[1m1 failed[0m;[0m Testing //rllib:examples/batch_norm_model_dqn_torch; 266s local ... (2 actions running)

[35mFLAKY: [0m//rllib:examples/attention_net_tf (Summary)
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_tf/test_attempts/attempt_1.log
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_tf/test_attempts/attempt_2.log
(05:17:54) [32mINFO: [0mFrom Testing //rllib:examples/attention_net_tf:
==================== Test output for //rllib:examples/attention_net_tf:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:03:54,562	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:03:57,094	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 3.0/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


[2m[36m(pid=2457)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=2457)[0m Model: "model_1"
[2m[36m(pid=2457)[0m __________________________________________________________________________________________________
[2m[36m(pid=2457)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=2457)[0m ==================================================================================================
[2m[36m(pid=2457)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=2457)[0m __________________________________________________________________________________________________
[2m[36m(pid=2457)[0m dense (Dense)                   (None, None, 64)     16448       inputs[0][0]                     
[2m[36m(pid=2457)[0m __________________________________________________________________________________________________
[2m[36m(pid=2457)[0m memory_in_0 (InputLayer)        [(None, None, 64)]   0                                            
[2m[36m(pid=2457)[0m __________________________________________________________________________________________________
[2m[36m(pid=2457)[0m mha_1 (SkipConnection)          (None, None, 64)     45376       dense[0][0]                      
[2m[36m(pid=2457)[0m                                                                  memory_in_0[0][0]                
[2m[36m(pid=2457)[0m __________________________________________________________________________________________________
[2m[36m(pid=2457)[0m pos_wise_mlp_1 (SkipConnection) (None, None, 64)     28960       mha_1[0][0]                      
[2m[36m(pid=2457)[0m ==================================================================================================
[2m[36m(pid=2457)[0m Total params: 90,784
[2m[36m(pid=2457)[0m Trainable params: 90,784
[2m[36m(pid=2457)[0m Non-trainable params: 0
[2m[36m(pid=2457)[0m __________________________________________________________________________________________________
Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=23.0,episode_reward_min=-31.0,episode_reward_mean=-1.85,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.5212242828672795, 'mean_inference_ms': 14.22697276025269, 'mean_action_processing_ms': 0.580323869316139, 'mean_env_wait_ms': 0.3999940198452318, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=4000,timers={'sample_time_ms': 3573.488, 'sample_throughput': 1119.354, 'learn_time_ms': 15406.736, 'learn_throughput': 259.627},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 0.018854866, 'policy_loss': 0.018227173, 'vf_loss': 43.751286, 'vf_explained_var': -0.021451507, 'kl': 0.004273376, 'entropy': 0.6644976, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 95.91785714285713, 'ram_util_percent': 56.564285714285724} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 4.2/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=23.0,episode_reward_min=-31.0,episode_reward_mean=0.35,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.674925719385276, 'mean_inference_ms': 14.067444941421929, 'mean_action_processing_ms': 0.5916460925723049, 'mean_env_wait_ms': 0.40011908451594824, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=8000,timers={'sample_time_ms': 3567.772, 'sample_throughput': 1121.148, 'learn_time_ms': 13871.33, 'learn_throughput': 288.365},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.042602297, 'policy_loss': -0.044084433, 'vf_loss': 27.076168, 'vf_explained_var': -0.014617334, 'kl': 0.018103633, 'entropy': 0.5989951, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 98.67391304347827, 'ram_util_percent': 56.817391304347815} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 4.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=27.0,episode_reward_min=-23.0,episode_reward_mean=4.1,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.7279955627213246, 'mean_inference_ms': 13.605976142031869, 'mean_action_processing_ms': 0.6021064734600395, 'mean_env_wait_ms': 0.3982228059601476, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=12000,timers={'sample_time_ms': 3360.64, 'sample_throughput': 1190.25, 'learn_time_ms': 13301.604, 'learn_throughput': 300.716},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.09071713, 'policy_loss': -0.092844635, 'vf_loss': 41.613796, 'vf_explained_var': 0.021322459, 'kl': 0.02266713, 'entropy': 0.5553506, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 97.82727272727271, 'ram_util_percent': 56.90000000000002} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 4.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=47.0,episode_reward_min=-15.0,episode_reward_mean=13.44,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.7055953166189455, 'mean_inference_ms': 12.9358271031603, 'mean_action_processing_ms': 0.6074475971075306, 'mean_env_wait_ms': 0.39340275232546545, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=16000,timers={'sample_time_ms': 3212.402, 'sample_throughput': 1245.174, 'learn_time_ms': 13223.215, 'learn_throughput': 302.498},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.18706277, 'policy_loss': -0.1897142, 'vf_loss': 101.55872, 'vf_explained_var': 0.13941814, 'kl': 0.014259112, 'entropy': 0.50301445, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000},perf={'cpu_util_percent': 98.27499999999999, 'ram_util_percent': 56.887499999999996} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 4.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=59.0,episode_reward_min=-13.0,episode_reward_mean=26.26,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.6881225968918896, 'mean_inference_ms': 12.580334050710292, 'mean_action_processing_ms': 0.6116046487533836, 'mean_env_wait_ms': 0.3879375920075477, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=20000,timers={'sample_time_ms': 3270.444, 'sample_throughput': 1223.076, 'learn_time_ms': 13155.089, 'learn_throughput': 304.065},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.21557143, 'policy_loss': -0.21996331, 'vf_loss': 243.76978, 'vf_explained_var': 0.14727882, 'kl': 0.016189551, 'entropy': 0.4742592, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 20000, 'num_agent_steps_sampled': 20000, 'num_steps_trained': 20000},perf={'cpu_util_percent': 98.83478260869565, 'ram_util_percent': 56.92173913043477} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 4.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=67.0,episode_reward_min=7.0,episode_reward_mean=39.3,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.6858802067503893, 'mean_inference_ms': 12.417379474220168, 'mean_action_processing_ms': 0.6126514178363639, 'mean_env_wait_ms': 0.38414169087858724, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=24000,timers={'sample_time_ms': 3199.118, 'sample_throughput': 1250.345, 'learn_time_ms': 13047.47, 'learn_throughput': 306.573},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.24032529, 'policy_loss': -0.24586353, 'vf_loss': 345.28485, 'vf_explained_var': 0.16847686, 'kl': 0.016619306, 'entropy': 0.40749544, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 24000, 'num_agent_steps_sampled': 24000, 'num_steps_trained': 24000},perf={'cpu_util_percent': 97.88260869565218, 'ram_util_percent': 57.052173913043454} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 4.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=77.0,episode_reward_min=15.0,episode_reward_mean=50.72,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.695008858766629, 'mean_inference_ms': 12.336329974008063, 'mean_action_processing_ms': 0.6132267588386384, 'mean_env_wait_ms': 0.3852799379649149, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=28000,timers={'sample_time_ms': 3204.842, 'sample_throughput': 1248.112, 'learn_time_ms': 12896.094, 'learn_throughput': 310.171},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.24253283, 'policy_loss': -0.2480824, 'vf_loss': 446.45068, 'vf_explained_var': 0.19728334, 'kl': 0.009733959, 'entropy': 0.37503627, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000},perf={'cpu_util_percent': 97.80454545454546, 'ram_util_percent': 56.46818181818182} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.4/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=83.0,episode_reward_min=33.0,episode_reward_mean=60.4,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.67812098554435, 'mean_inference_ms': 12.156208287320004, 'mean_action_processing_ms': 0.6039169955189361, 'mean_env_wait_ms': 0.3798353874766838, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=32000,timers={'sample_time_ms': 3127.644, 'sample_throughput': 1278.918, 'learn_time_ms': 12570.734, 'learn_throughput': 318.199},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.24128632, 'policy_loss': -0.24756463, 'vf_loss': 541.58606, 'vf_explained_var': 0.21405002, 'kl': 0.007940425, 'entropy': 0.32861716, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 32000, 'num_agent_steps_sampled': 32000, 'num_steps_trained': 32000},perf={'cpu_util_percent': 93.39444444444445, 'ram_util_percent': 40.01666666666667} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 2.9/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=87.0,episode_reward_min=43.0,episode_reward_mean=67.02,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.638407325165574, 'mean_inference_ms': 11.946709439087522, 'mean_action_processing_ms': 0.5921924037568018, 'mean_env_wait_ms': 0.37058045145414165, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=36000,timers={'sample_time_ms': 3065.523, 'sample_throughput': 1304.834, 'learn_time_ms': 12439.85, 'learn_throughput': 321.547},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.2554374, 'policy_loss': -0.26164666, 'vf_loss': 542.482, 'vf_explained_var': 0.25367865, 'kl': 0.007166619, 'entropy': 0.2905736, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 36000, 'num_agent_steps_sampled': 36000, 'num_steps_trained': 36000},perf={'cpu_util_percent': 97.55238095238097, 'ram_util_percent': 40.185714285714276} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.2/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=87.0,episode_reward_min=49.0,episode_reward_mean=72.98,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.580556842985985, 'mean_inference_ms': 11.706148228862512, 'mean_action_processing_ms': 0.5814814828216052, 'mean_env_wait_ms': 0.3608429017985793, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=40000,timers={'sample_time_ms': 3006.05, 'sample_throughput': 1330.65, 'learn_time_ms': 12395.465, 'learn_throughput': 322.699},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.2503859, 'policy_loss': -0.2566321, 'vf_loss': 563.52246, 'vf_explained_var': 0.2902093, 'kl': 0.005721728, 'entropy': 0.24730061, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000},perf={'cpu_util_percent': 98.61428571428571, 'ram_util_percent': 45.36190476190476} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.28 GiB heap, 0.0/1.64 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_abb3c_00000 reported episode_reward_max=91.0,episode_reward_min=49.0,episode_reward_mean=77.26,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.52743794676915, 'mean_inference_ms': 11.519430350453995, 'mean_action_processing_ms': 0.5753780590009007, 'mean_env_wait_ms': 0.3559816013890194, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=44000,timers={'sample_time_ms': 2910.424, 'sample_throughput': 1374.37, 'learn_time_ms': 12021.766, 'learn_throughput': 332.73},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.15000000596046448, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.24717668, 'policy_loss': -0.253266, 'vf_loss': 515.8137, 'vf_explained_var': 0.33092052, 'kl': 0.007836062, 'entropy': 0.24419811, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 44000, 'num_agent_steps_sampled': 44000, 'num_steps_trained': 44000},perf={'cpu_util_percent': 97.64761904761905, 'ram_util_percent': 49.019047619047626} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.-- Test timed out at 2021-04-09 12:07:31 UTC --
[2m[36m(pid=2457)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=2457)[0m Instructions for updating:
[2m[36m(pid=2457)[0m non-resource variables are not supported in the long term
[2m[36m(pid=2457)[0m 2021-04-09 12:04:07,181	INFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=2457)[0m 2021-04-09 12:04:07,181	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=2457)[0m 2021-04-09 12:04:26,175	INFO trainable.py:104 -- Trainable.setup took 18.995 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=2457)[0m 2021-04-09 12:04:26,176	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=2457)[0m 2021-04-09 12:04:29,754	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=2457)[0m 2021-04-09 12:04:29,754	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
[2m[36m(pid=2457)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:943: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=2457)[0m Instructions for updating:
[2m[36m(pid=2457)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
2021-04-09 12:07:32,006	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 12:07:32,036	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
2021-04-09 12:07:32,037	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 12:07:32,037	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
================================================================================
==================== Test output for //rllib:examples/attention_net_tf:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:07:57,147	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:08:00,697	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 3.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


[2m[36m(pid=3124)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=3124)[0m Model: "model_1"
[2m[36m(pid=3124)[0m __________________________________________________________________________________________________
[2m[36m(pid=3124)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=3124)[0m ==================================================================================================
[2m[36m(pid=3124)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=3124)[0m __________________________________________________________________________________________________
[2m[36m(pid=3124)[0m dense (Dense)                   (None, None, 64)     16448       inputs[0][0]                     
[2m[36m(pid=3124)[0m __________________________________________________________________________________________________
[2m[36m(pid=3124)[0m memory_in_0 (InputLayer)        [(None, None, 64)]   0                                            
[2m[36m(pid=3124)[0m __________________________________________________________________________________________________
[2m[36m(pid=3124)[0m mha_1 (SkipConnection)          (None, None, 64)     45376       dense[0][0]                      
[2m[36m(pid=3124)[0m                                                                  memory_in_0[0][0]                
[2m[36m(pid=3124)[0m __________________________________________________________________________________________________
[2m[36m(pid=3124)[0m pos_wise_mlp_1 (SkipConnection) (None, None, 64)     28960       mha_1[0][0]                      
[2m[36m(pid=3124)[0m ==================================================================================================
[2m[36m(pid=3124)[0m Total params: 90,784
[2m[36m(pid=3124)[0m Trainable params: 90,784
[2m[36m(pid=3124)[0m Non-trainable params: 0
[2m[36m(pid=3124)[0m __________________________________________________________________________________________________
Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=21.0,episode_reward_min=-23.0,episode_reward_mean=-0.25,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.3688356674725735, 'mean_inference_ms': 11.542874189158576, 'mean_action_processing_ms': 0.5373065151385407, 'mean_env_wait_ms': 0.309609655124038, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=4000,timers={'sample_time_ms': 2981.932, 'sample_throughput': 1341.412, 'learn_time_ms': 12774.835, 'learn_throughput': 313.116},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 0.03554508, 'policy_loss': 0.034606926, 'vf_loss': 29.870518, 'vf_explained_var': -0.047543228, 'kl': 0.006290354, 'entropy': 0.6186158, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 98.29583333333333, 'ram_util_percent': 41.225} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=25.0,episode_reward_min=-23.0,episode_reward_mean=-0.35,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.4191769528224216, 'mean_inference_ms': 11.702074872768705, 'mean_action_processing_ms': 0.5567621926519128, 'mean_env_wait_ms': 0.31093064775047, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=8000,timers={'sample_time_ms': 3061.437, 'sample_throughput': 1306.576, 'learn_time_ms': 12370.043, 'learn_throughput': 323.362},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.0010253879, 'policy_loss': -0.002973966, 'vf_loss': 31.890644, 'vf_explained_var': -0.036405914, 'kl': 0.0112391235, 'entropy': 0.6181679, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 98.07727272727271, 'ram_util_percent': 47.41363636363636} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=33.0,episode_reward_min=-23.0,episode_reward_mean=3.22,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.4293975588838763, 'mean_inference_ms': 11.79092732187695, 'mean_action_processing_ms': 0.5819115117743451, 'mean_env_wait_ms': 0.32406568350021614, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=12000,timers={'sample_time_ms': 3056.625, 'sample_throughput': 1308.633, 'learn_time_ms': 12156.223, 'learn_throughput': 329.05},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.073367886, 'policy_loss': -0.075327985, 'vf_loss': 47.55337, 'vf_explained_var': -0.007327827, 'kl': 0.010433369, 'entropy': 0.6020919, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 97.42857142857143, 'ram_util_percent': 48.82380952380952} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.2/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=33.0,episode_reward_min=-23.0,episode_reward_mean=9.04,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.349690790672415, 'mean_inference_ms': 11.68936260054566, 'mean_action_processing_ms': 0.5846200012146818, 'mean_env_wait_ms': 0.3326016918748277, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=16000,timers={'sample_time_ms': 2923.558, 'sample_throughput': 1368.196, 'learn_time_ms': 11942.479, 'learn_throughput': 334.939},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.1565944, 'policy_loss': -0.15953776, 'vf_loss': 69.08121, 'vf_explained_var': 0.0445813, 'kl': 0.014113321, 'entropy': 0.5701192, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000},perf={'cpu_util_percent': 96.38999999999999, 'ram_util_percent': 38.655} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 2.9/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=45.0,episode_reward_min=-9.0,episode_reward_mean=17.9,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.240196805242901, 'mean_inference_ms': 11.443930368236472, 'mean_action_processing_ms': 0.5719431833327042, 'mean_env_wait_ms': 0.3334924270003697, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=20000,timers={'sample_time_ms': 2869.821, 'sample_throughput': 1393.815, 'learn_time_ms': 11934.603, 'learn_throughput': 335.16},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.21693406, 'policy_loss': -0.22073211, 'vf_loss': 122.92443, 'vf_explained_var': 0.099911645, 'kl': 0.015399092, 'entropy': 0.5109955, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 20000, 'num_agent_steps_sampled': 20000, 'num_steps_trained': 20000},perf={'cpu_util_percent': 98.23333333333332, 'ram_util_percent': 41.63333333333334} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=53.0,episode_reward_min=-1.0,episode_reward_mean=26.4,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.2121378308644464, 'mean_inference_ms': 11.422571534214748, 'mean_action_processing_ms': 0.5651854341569218, 'mean_env_wait_ms': 0.33252106167866846, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=24000,timers={'sample_time_ms': 2958.161, 'sample_throughput': 1352.191, 'learn_time_ms': 11899.024, 'learn_throughput': 336.162},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.23209432, 'policy_loss': -0.23594737, 'vf_loss': 176.10107, 'vf_explained_var': 0.13650593, 'kl': 0.012915938, 'entropy': 0.4911772, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 24000, 'num_agent_steps_sampled': 24000, 'num_steps_trained': 24000},perf={'cpu_util_percent': 98.2, 'ram_util_percent': 47.373913043478254} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=61.0,episode_reward_min=9.0,episode_reward_mean=35.92,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.2277436554875423, 'mean_inference_ms': 11.498989206149412, 'mean_action_processing_ms': 0.5676094396497792, 'mean_env_wait_ms': 0.3335132360673403, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=28000,timers={'sample_time_ms': 2942.19, 'sample_throughput': 1359.532, 'learn_time_ms': 12021.169, 'learn_throughput': 332.746},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.24444233, 'policy_loss': -0.24916984, 'vf_loss': 280.65973, 'vf_explained_var': 0.16996345, 'kl': 0.011836891, 'entropy': 0.44647783, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000},perf={'cpu_util_percent': 97.8090909090909, 'ram_util_percent': 49.75909090909091} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=77.0,episode_reward_min=17.0,episode_reward_mean=46.52,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.2296174484453206, 'mean_inference_ms': 11.533126851362736, 'mean_action_processing_ms': 0.5687043386887326, 'mean_env_wait_ms': 0.33270918811416084, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=32000,timers={'sample_time_ms': 2930.37, 'sample_throughput': 1365.015, 'learn_time_ms': 11601.992, 'learn_throughput': 344.768},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.25337484, 'policy_loss': -0.259479, 'vf_loss': 395.76025, 'vf_explained_var': 0.17740287, 'kl': 0.012640902, 'entropy': 0.38161373, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 32000, 'num_agent_steps_sampled': 32000, 'num_steps_trained': 32000},perf={'cpu_util_percent': 91.72352941176469, 'ram_util_percent': 40.388235294117656} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 2.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=77.0,episode_reward_min=25.0,episode_reward_mean=55.86,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.1745861657501533, 'mean_inference_ms': 11.355916544412672, 'mean_action_processing_ms': 0.5592502415351136, 'mean_env_wait_ms': 0.32562802707199096, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=36000,timers={'sample_time_ms': 2824.235, 'sample_throughput': 1416.313, 'learn_time_ms': 11440.981, 'learn_throughput': 349.62},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.2501248, 'policy_loss': -0.25543976, 'vf_loss': 480.9059, 'vf_explained_var': 0.20763402, 'kl': 0.004343794, 'entropy': 0.36284685, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 36000, 'num_agent_steps_sampled': 36000, 'num_steps_trained': 36000},perf={'cpu_util_percent': 91.69444444444444, 'ram_util_percent': 37.044444444444444} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 2.9/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=77.0,episode_reward_min=43.0,episode_reward_mean=62.24,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.120998514944188, 'mean_inference_ms': 11.137518528308542, 'mean_action_processing_ms': 0.5502897885691965, 'mean_env_wait_ms': 0.3187986673517176, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=40000,timers={'sample_time_ms': 2791.203, 'sample_throughput': 1433.074, 'learn_time_ms': 11417.114, 'learn_throughput': 350.351},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.25351778, 'policy_loss': -0.2582121, 'vf_loss': 422.3796, 'vf_explained_var': 0.22965565, 'kl': 0.00798932, 'entropy': 0.32844284, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000},perf={'cpu_util_percent': 97.955, 'ram_util_percent': 40.61} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.2/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=83.0,episode_reward_min=45.0,episode_reward_mean=66.92,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.113248091182584, 'mean_inference_ms': 11.064460983196286, 'mean_action_processing_ms': 0.5459911361370061, 'mean_env_wait_ms': 0.31535087832876224, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=44000,timers={'sample_time_ms': 2817.251, 'sample_throughput': 1419.824, 'learn_time_ms': 11356.368, 'learn_throughput': 352.225},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.25825983, 'policy_loss': -0.26378563, 'vf_loss': 486.8999, 'vf_explained_var': 0.26904184, 'kl': 0.009731991, 'entropy': 0.3163769, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 44000, 'num_agent_steps_sampled': 44000, 'num_steps_trained': 44000},perf={'cpu_util_percent': 98.1909090909091, 'ram_util_percent': 46.06818181818182} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=87.0,episode_reward_min=53.0,episode_reward_mean=69.7,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.1322390727271956, 'mean_inference_ms': 11.092085849019252, 'mean_action_processing_ms': 0.5476786924720387, 'mean_env_wait_ms': 0.315704313132498, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=48000,timers={'sample_time_ms': 2785.525, 'sample_throughput': 1435.995, 'learn_time_ms': 11263.855, 'learn_throughput': 355.118},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.05970017, 'policy_loss': -0.062839694, 'vf_loss': 310.3406, 'vf_explained_var': 0.38290295, 'kl': 0.0034777434, 'entropy': 0.31166682, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 48000, 'num_agent_steps_sampled': 48000, 'num_steps_trained': 48000},perf={'cpu_util_percent': 97.73, 'ram_util_percent': 49.38} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=87.0,episode_reward_min=53.0,episode_reward_mean=70.84,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.1567911600599756, 'mean_inference_ms': 11.11676535309676, 'mean_action_processing_ms': 0.5520596012724674, 'mean_env_wait_ms': 0.3175440220148833, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=52000,timers={'sample_time_ms': 2773.705, 'sample_throughput': 1442.115, 'learn_time_ms': 11113.982, 'learn_throughput': 359.907},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.05000000074505806, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.063546084, 'policy_loss': -0.06713261, 'vf_loss': 327.96228, 'vf_explained_var': 0.39600474, 'kl': 0.01184458, 'entropy': 0.28532776, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 52000, 'num_agent_steps_sampled': 52000, 'num_steps_trained': 52000},perf={'cpu_util_percent': 95.93157894736842, 'ram_util_percent': 46.55263157894738} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 2.8/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=87.0,episode_reward_min=53.0,episode_reward_mean=71.78,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.1458941811323844, 'mean_inference_ms': 11.061283788913643, 'mean_action_processing_ms': 0.5514106417619334, 'mean_env_wait_ms': 0.3161521004256867, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=56000,timers={'sample_time_ms': 2746.622, 'sample_throughput': 1456.334, 'learn_time_ms': 10777.938, 'learn_throughput': 371.129},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.05000000074505806, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.07084894, 'policy_loss': -0.07476419, 'vf_loss': 317.06082, 'vf_explained_var': 0.3929238, 'kl': 0.020803504, 'entropy': 0.29553366, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 56000, 'num_agent_steps_sampled': 56000, 'num_steps_trained': 56000},perf={'cpu_util_percent': 88.29333333333332, 'ram_util_percent': 36.7} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.
== Status ==
Memory usage on this node: 2.8/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.82 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9b0d1275c9f481b38bfc121c3341a5b3/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_3c617_00000 reported episode_reward_max=87.0,episode_reward_min=61.0,episode_reward_mean=73.18,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.107256283983492, 'mean_inference_ms': 10.931735430715388, 'mean_action_processing_ms': 0.5434689902338927, 'mean_env_wait_ms': 0.31092028628292256, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=60000,timers={'sample_time_ms': 2681.241, 'sample_throughput': 1491.847, 'learn_time_ms': 10669.806, 'learn_throughput': 374.89},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.07500000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': -0.03190644, 'policy_loss': -0.035056353, 'vf_loss': 259.331, 'vf_explained_var': 0.36082125, 'kl': 0.011107858, 'entropy': 0.27648866, 'entropy_coeff': 0.0010000000474974513, 'model': {}}}}), 'num_steps_sampled': 60000, 'num_agent_steps_sampled': 60000, 'num_steps_trained': 60000},perf={'cpu_util_percent': 94.11578947368422, 'ram_util_percent': 38.06315789473684} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'tf'}.-- Test timed out at 2021-04-09 12:12:47 UTC --
[2m[36m(pid=3124)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=3124)[0m Instructions for updating:
[2m[36m(pid=3124)[0m non-resource variables are not supported in the long term
[2m[36m(pid=3124)[0m 2021-04-09 12:08:14,061	INFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=3124)[0m 2021-04-09 12:08:14,061	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=3124)[0m 2021-04-09 12:08:20,904	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=3124)[0m 2021-04-09 12:08:23,889	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=3124)[0m 2021-04-09 12:08:23,889	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
[2m[36m(pid=3124)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/tf_policy.py:943: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=3124)[0m Instructions for updating:
[2m[36m(pid=3124)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
2021-04-09 12:12:47,135	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 12:12:47,208	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 12:12:47,208	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
2021-04-09 12:12:47,221	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
================================================================================
(05:19:41) [32m[35 / 37][0m 6 / 10 tests, [31m[1m1 failed[0m;[0m Testing //rllib:examples/batch_norm_model_dqn_torch; 405s local ... (2 actions running)
(05:22:21) [32m[36 / 38][0m 7 / 10 tests, [31m[1m1 failed[0m;[0m Testing //rllib:examples/batch_norm_model_dqn_torch; 564s local ... (2 actions running)
(05:25:24) [32m[41 / 42][0m 9 / 10 tests, [31m[1m1 failed[0m;[0m Testing //rllib:examples/attention_net_torch; 307s local
(05:30:01) [32m[41 / 42][0m 9 / 10 tests, [31m[1m1 failed[0m;[0m Testing //rllib:examples/attention_net_torch; 269s local

[35mFLAKY: [0m//rllib:examples/attention_net_torch (Summary)
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_torch/test_attempts/attempt_1.log
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_torch/test_attempts/attempt_2.log
(05:33:30) [32mINFO: [0mFrom Testing //rllib:examples/attention_net_torch:
==================== Test output for //rllib:examples/attention_net_torch:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:20:23,592	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:20:26,231	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8267[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 3.4/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


[2m[36m(pid=5584)[0m Loading environment football failed: No module named 'gfootball'
Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=21.0,episode_reward_min=-13.0,episode_reward_mean=1.5,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.8803194340188703, 'mean_inference_ms': 12.930791769454725, 'mean_action_processing_ms': 0.6381362231809701, 'mean_env_wait_ms': 0.4005218619730934, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=4000,timers={'sample_time_ms': 3399.28, 'sample_throughput': 1176.72, 'learn_time_ms': 15058.592, 'learn_throughput': 265.629},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.030367108617121212, 'policy_loss': -0.03092255322740246, 'vf_loss': 24.89617444307376, 'vf_explained_var': -0.0058664265, 'kl': 0.004234858424378893, 'entropy': 0.5404901955372248, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 99.76666666666668, 'ram_util_percent': 48.911111111111104} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=21.0,episode_reward_min=-17.0,episode_reward_mean=1.925,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.7764322349485955, 'mean_inference_ms': 12.318329123891264, 'mean_action_processing_ms': 0.6068003928829508, 'mean_env_wait_ms': 0.38922935636627365, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=8000,timers={'sample_time_ms': 3083.198, 'sample_throughput': 1297.354, 'learn_time_ms': 15068.552, 'learn_throughput': 265.454},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.10000000000000002, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.05438929688758575, 'policy_loss': -0.05483394672568792, 'vf_loss': 31.851713755191902, 'vf_explained_var': 0.0022920035, 'kl': 0.006707198190269992, 'entropy': 0.5445867662246411, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 99.77600000000001, 'ram_util_percent': 49.083999999999996} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=21.0,episode_reward_min=-19.0,episode_reward_mean=1.8,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.684424099074749, 'mean_inference_ms': 11.759006298252661, 'mean_action_processing_ms': 0.5794315887216662, 'mean_env_wait_ms': 0.37817828048759927, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=12000,timers={'sample_time_ms': 2961.655, 'sample_throughput': 1350.596, 'learn_time_ms': 14967.991, 'learn_throughput': 267.237},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.10000000000000002, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.036178443485345595, 'policy_loss': 0.035986544039004885, 'vf_loss': 30.913883802218315, 'vf_explained_var': -0.00013790987, 'kl': 0.004053630288511228, 'entropy': 0.5226064622402191, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 99.80400000000002, 'ram_util_percent': 49.19200000000001} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=21.0,episode_reward_min=-29.0,episode_reward_mean=0.48,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.601831038005284, 'mean_inference_ms': 11.25221287114362, 'mean_action_processing_ms': 0.55293690206496, 'mean_env_wait_ms': 0.3668843783100011, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=16000,timers={'sample_time_ms': 2914.988, 'sample_throughput': 1372.218, 'learn_time_ms': 14981.623, 'learn_throughput': 266.994},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.046001009117716395, 'policy_loss': 0.04576901145852529, 'vf_loss': 33.23440052912785, 'vf_explained_var': -0.0010033876, 'kl': 0.00942967945518784, 'entropy': 0.5718268981346717, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000},perf={'cpu_util_percent': 99.81153846153848, 'ram_util_percent': 49.28846153846153} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=21.0,episode_reward_min=-29.0,episode_reward_mean=-0.74,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.5686158792454195, 'mean_inference_ms': 11.0296924362214, 'mean_action_processing_ms': 0.5403137491293374, 'mean_env_wait_ms': 0.3616230212251429, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=20000,timers={'sample_time_ms': 2881.038, 'sample_throughput': 1388.389, 'learn_time_ms': 15025.886, 'learn_throughput': 266.207},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.031056100359329812, 'policy_loss': -0.031160940416157246, 'vf_loss': 22.79040302374424, 'vf_explained_var': 0.0012065646, 'kl': 0.009092840676506361, 'entropy': 0.5777006577222775, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 20000, 'num_agent_steps_sampled': 20000, 'num_steps_trained': 20000},perf={'cpu_util_percent': 99.83200000000001, 'ram_util_percent': 49.376000000000005} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=29.0,episode_reward_min=-29.0,episode_reward_mean=1.42,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.6023492442968386, 'mean_inference_ms': 10.94911720673319, 'mean_action_processing_ms': 0.5371160609939429, 'mean_env_wait_ms': 0.36142787571609447, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=24000,timers={'sample_time_ms': 2911.212, 'sample_throughput': 1373.998, 'learn_time_ms': 15065.872, 'learn_throughput': 265.501},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.03389039874458925, 'policy_loss': -0.03463728047716312, 'vf_loss': 34.39960078704051, 'vf_explained_var': 0.014299302, 'kl': 0.018737782652561482, 'entropy': 0.5340065084970914, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 24000, 'num_agent_steps_sampled': 24000, 'num_steps_trained': 24000},perf={'cpu_util_percent': 99.84444444444445, 'ram_util_percent': 49.43333333333334} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=35.0,episode_reward_min=-15.0,episode_reward_mean=8.06,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.6353722000600057, 'mean_inference_ms': 10.899570429772252, 'mean_action_processing_ms': 0.5375991921077199, 'mean_env_wait_ms': 0.36070434184697836, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=28000,timers={'sample_time_ms': 2888.074, 'sample_throughput': 1385.006, 'learn_time_ms': 15117.995, 'learn_throughput': 264.585},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.11767103378541577, 'policy_loss': -0.11860159053825416, 'vf_loss': 58.016083375001564, 'vf_explained_var': 0.053682327, 'kl': 0.016386445134113997, 'entropy': 0.46891481677691144, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000},perf={'cpu_util_percent': 99.7846153846154, 'ram_util_percent': 49.55769230769231} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=53.0,episode_reward_min=-15.0,episode_reward_mean=15.08,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.638772499818396, 'mean_inference_ms': 10.857621997119232, 'mean_action_processing_ms': 0.53807650918072, 'mean_env_wait_ms': 0.3614367397216279, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=32000,timers={'sample_time_ms': 2875.848, 'sample_throughput': 1390.894, 'learn_time_ms': 14962.402, 'learn_throughput': 267.337},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.17709039342709076, 'policy_loss': -0.1784843994447818, 'vf_loss': 103.95004702837039, 'vf_explained_var': 0.115025885, 'kl': 0.015307190875785474, 'entropy': 0.4108440547417372, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 32000, 'num_agent_steps_sampled': 32000, 'num_steps_trained': 32000},perf={'cpu_util_percent': 97.6304347826087, 'ram_util_percent': 46.32608695652175} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=53.0,episode_reward_min=-13.0,episode_reward_mean=23.26,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.604846112170656, 'mean_inference_ms': 10.765456402527638, 'mean_action_processing_ms': 0.5329272349537904, 'mean_env_wait_ms': 0.36066881783811, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=36000,timers={'sample_time_ms': 2832.216, 'sample_throughput': 1412.322, 'learn_time_ms': 14993.922, 'learn_throughput': 266.775},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.17931145725724024, 'policy_loss': -0.18097741386065117, 'vf_loss': 154.12668042305188, 'vf_explained_var': 0.15291825, 'kl': 0.009870208429697996, 'entropy': 0.3688294490178426, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 36000, 'num_agent_steps_sampled': 36000, 'num_steps_trained': 36000},perf={'cpu_util_percent': 98.15384615384616, 'ram_util_percent': 42.37307692307692} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=53.0,episode_reward_min=-1.0,episode_reward_mean=30.34,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.579402923298131, 'mean_inference_ms': 10.68481536749704, 'mean_action_processing_ms': 0.5266713961240361, 'mean_env_wait_ms': 0.35888075830010707, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=40000,timers={'sample_time_ms': 2823.863, 'sample_throughput': 1416.499, 'learn_time_ms': 15056.281, 'learn_throughput': 265.67},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.19690925052437264, 'policy_loss': -0.19854778434651402, 'vf_loss': 170.7591361510448, 'vf_explained_var': 0.19695248, 'kl': 0.005596277280113636, 'entropy': 0.34887241476621383, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000},perf={'cpu_util_percent': 99.83846153846154, 'ram_util_percent': 47.584615384615375} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=53.0,episode_reward_min=-1.0,episode_reward_mean=31.82,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.596833944923193, 'mean_inference_ms': 10.663994713600692, 'mean_action_processing_ms': 0.5238569491388846, 'mean_env_wait_ms': 0.3573899810527143, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=44000,timers={'sample_time_ms': 2794.388, 'sample_throughput': 1431.441, 'learn_time_ms': 15108.276, 'learn_throughput': 264.756},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05000000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.19368245968451867, 'policy_loss': -0.19512801330823165, 'vf_loss': 139.8053114964412, 'vf_explained_var': 0.2222178, 'kl': 0.0066260165200592615, 'entropy': 0.28380190485563034, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 44000, 'num_agent_steps_sampled': 44000, 'num_steps_trained': 44000},perf={'cpu_util_percent': 99.84444444444445, 'ram_util_percent': 47.659259259259265} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=61.0,episode_reward_min=7.0,episode_reward_mean=36.18,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.624214324580088, 'mean_inference_ms': 10.694931254274424, 'mean_action_processing_ms': 0.5267679890480238, 'mean_env_wait_ms': 0.3578966050230721, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=48000,timers={'sample_time_ms': 2813.314, 'sample_throughput': 1421.811, 'learn_time_ms': 14974.105, 'learn_throughput': 267.128},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.05, 'cur_lr': 5e-05, 'total_loss': -0.03648616167519129, 'policy_loss': -0.03812246493092089, 'vf_loss': 169.15454598629114, 'vf_explained_var': 0.30385953, 'kl': 0.0036374108625971007, 'entropy': 0.2371091196934382, 'entropy_coeff': 0.001}}}), 'num_steps_sampled': 48000, 'num_agent_steps_sampled': 48000, 'num_steps_trained': 48000},perf={'cpu_util_percent': 99.76666666666667, 'ram_util_percent': 47.70000000000001} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=61.0,episode_reward_min=7.0,episode_reward_mean=38.1,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.637255763330174, 'mean_inference_ms': 10.712945915684331, 'mean_action_processing_ms': 0.5290013848252652, 'mean_env_wait_ms': 0.3589186023875419, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=52000,timers={'sample_time_ms': 2820.767, 'sample_throughput': 1418.054, 'learn_time_ms': 14860.682, 'learn_throughput': 269.167},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.025, 'cur_lr': 5e-05, 'total_loss': -0.05835921105674722, 'policy_loss': -0.059596176484994816, 'vf_loss': 128.6458244901715, 'vf_explained_var': 0.32425642, 'kl': 0.0064297059099331045, 'entropy': 0.21022756533189255, 'entropy_coeff': 0.001}}}), 'num_steps_sampled': 52000, 'num_agent_steps_sampled': 52000, 'num_steps_trained': 52000},perf={'cpu_util_percent': 99.84782608695652, 'ram_util_percent': 47.70000000000002} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=63.0,episode_reward_min=11.0,episode_reward_mean=41.22,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.6361151347334784, 'mean_inference_ms': 10.705224886134937, 'mean_action_processing_ms': 0.5297176936480722, 'mean_env_wait_ms': 0.35913468168619217, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=56000,timers={'sample_time_ms': 2823.23, 'sample_throughput': 1416.817, 'learn_time_ms': 14784.983, 'learn_throughput': 270.545},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.025, 'cur_lr': 5e-05, 'total_loss': -0.040886409637151344, 'policy_loss': -0.04227586722735203, 'vf_loss': 147.5300505667022, 'vf_explained_var': 0.35455975, 'kl': 0.004358865652289806, 'entropy': 0.19481268886363867, 'entropy_coeff': 0.001}}}), 'num_steps_sampled': 56000, 'num_agent_steps_sampled': 56000, 'num_steps_trained': 56000},perf={'cpu_util_percent': 99.91249999999998, 'ram_util_percent': 47.7125} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 3.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=73.0,episode_reward_min=17.0,episode_reward_mean=43.34,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.6324460132312266, 'mean_inference_ms': 10.686255240339797, 'mean_action_processing_ms': 0.5309436697317502, 'mean_env_wait_ms': 0.35905322305893095, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=60000,timers={'sample_time_ms': 2826.466, 'sample_throughput': 1415.195, 'learn_time_ms': 14624.11, 'learn_throughput': 273.521},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.012500000000000002, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.06234913481733738, 'policy_loss': -0.06366461503486602, 'vf_loss': 147.8872582851312, 'vf_explained_var': 0.32655728, 'kl': 0.002523170960231278, 'entropy': 0.19493282223359132, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 60000, 'num_agent_steps_sampled': 60000, 'num_steps_trained': 60000},perf={'cpu_util_percent': 95.34166666666665, 'ram_util_percent': 45.975} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/1.41 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_f87b2_00000 reported episode_reward_max=73.0,episode_reward_min=17.0,episode_reward_mean=45.04,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 2.596587325998462, 'mean_inference_ms': 10.583414011120965, 'mean_action_processing_ms': 0.5268882022630474, 'mean_env_wait_ms': 0.3549768185223084, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=64000,timers={'sample_time_ms': 2697.267, 'sample_throughput': 1482.983, 'learn_time_ms': 13967.519, 'learn_throughput': 286.379},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.006250000000000001, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.05785630320986876, 'policy_loss': -0.05896361141155163, 'vf_loss': 123.31998609885191, 'vf_explained_var': 0.35730997, 'kl': 0.004944142742225757, 'entropy': 0.15679354583605742, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 64000, 'num_agent_steps_sampled': 64000, 'num_steps_trained': 64000},perf={'cpu_util_percent': 60.38666666666666, 'ram_util_percent': 35.08000000000001} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.-- Test timed out at 2021-04-09 12:25:16 UTC --
[2m[36m(pid=5584)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=5584)[0m Instructions for updating:
[2m[36m(pid=5584)[0m non-resource variables are not supported in the long term
[2m[36m(pid=5584)[0m 2021-04-09 12:20:35,622	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=5584)[0m /home/ray/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[2m[36m(pid=5584)[0m   return torch._C._cuda_getDeviceCount() > 0
[2m[36m(pid=5584)[0m 2021-04-09 12:20:36,041	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=5584)[0m 2021-04-09 12:20:39,447	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=5584)[0m 2021-04-09 12:20:39,447	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
2021-04-09 12:25:16,987	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 12:25:16,988	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 12:25:16,993	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
2021-04-09 12:25:16,997	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
================================================================================
==================== Test output for //rllib:examples/attention_net_torch:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:25:35,643	INFO utils.py:497 -- Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using multiprocessing.cpu_count() to detect the number of CPUs. This may be inconsistent when used inside docker. To correctly detect CPUs, unset the env var: `RAY_USE_MULTIPROCESSING_CPU_COUNT`.
2021-04-09 12:25:37,223	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 2.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


[2m[36m(pid=6187)[0m Loading environment football failed: No module named 'gfootball'
Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=17.0,episode_reward_min=-31.0,episode_reward_mean=-3.4,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3004665944113658, 'mean_inference_ms': 5.82663574028964, 'mean_action_processing_ms': 0.3031047422494461, 'mean_env_wait_ms': 0.1733113284134746, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=4000,timers={'sample_time_ms': 1534.649, 'sample_throughput': 2606.458, 'learn_time_ms': 8491.744, 'learn_throughput': 471.046},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.017299096685093947, 'policy_loss': 0.015982370847501814, 'vf_loss': 34.19332514053736, 'vf_explained_var': -0.022619367, 'kl': 0.007876466926879799, 'entropy': 0.6004971984105233, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000},perf={'cpu_util_percent': 59.153333333333336, 'ram_util_percent': 34.86666666666666} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=21.0,episode_reward_min=-31.0,episode_reward_mean=-2.2,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3296441983585627, 'mean_inference_ms': 5.99215194721411, 'mean_action_processing_ms': 0.31066436915241996, 'mean_env_wait_ms': 0.178568701970604, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=8000,timers={'sample_time_ms': 1612.32, 'sample_throughput': 2480.897, 'learn_time_ms': 8482.022, 'learn_throughput': 471.586},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.024438745509355497, 'policy_loss': -0.02613243431999133, 'vf_loss': 28.21616272437267, 'vf_explained_var': -0.0019159272, 'kl': 0.010095674849043671, 'entropy': 0.6075959801673889, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 8000, 'num_agent_steps_sampled': 8000, 'num_steps_trained': 8000},perf={'cpu_util_percent': 60.47857142857141, 'ram_util_percent': 34.99285714285714} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=27.0,episode_reward_min=-31.0,episode_reward_mean=-0.38,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3874168668770503, 'mean_inference_ms': 6.3881239759859625, 'mean_action_processing_ms': 0.3257526649178981, 'mean_env_wait_ms': 0.18865250007960163, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=12000,timers={'sample_time_ms': 1788.096, 'sample_throughput': 2237.016, 'learn_time_ms': 8691.063, 'learn_throughput': 460.243},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.011774617354743756, 'policy_loss': -0.012600848200516058, 'vf_loss': 36.01212969804421, 'vf_explained_var': -0.005814881, 'kl': 0.005237469097408347, 'entropy': 0.5813816632979956, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 12000, 'num_agent_steps_sampled': 12000, 'num_steps_trained': 12000},perf={'cpu_util_percent': 68.04374999999999, 'ram_util_percent': 35.3} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=29.0,episode_reward_min=-19.0,episode_reward_mean=3.3,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4194071615321915, 'mean_inference_ms': 6.668422262413574, 'mean_action_processing_ms': 0.33557197835761715, 'mean_env_wait_ms': 0.195876957092802, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=16000,timers={'sample_time_ms': 1732.81, 'sample_throughput': 2308.389, 'learn_time_ms': 8787.539, 'learn_throughput': 455.19},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.07738149056258874, 'policy_loss': -0.07977477504083744, 'vf_loss': 43.092154380602715, 'vf_explained_var': 0.0043890355, 'kl': 0.012518669192034464, 'entropy': 0.5413802151496594, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000},perf={'cpu_util_percent': 63.99375, 'ram_util_percent': 35.25} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=45.0,episode_reward_min=-17.0,episode_reward_mean=7.88,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.453330671208649, 'mean_inference_ms': 6.763347485444886, 'mean_action_processing_ms': 0.3365012794442724, 'mean_env_wait_ms': 0.19790418931006964, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=20000,timers={'sample_time_ms': 1763.553, 'sample_throughput': 2268.148, 'learn_time_ms': 8811.48, 'learn_throughput': 453.953},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.16305206892773128, 'policy_loss': -0.16726876709323663, 'vf_loss': 62.13696337968875, 'vf_explained_var': 0.029591689, 'kl': 0.02046817135161314, 'entropy': 0.49831144244242936, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 20000, 'num_agent_steps_sampled': 20000, 'num_steps_trained': 20000},perf={'cpu_util_percent': 63.48666666666667, 'ram_util_percent': 35.339999999999996} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=45.0,episode_reward_min=-9.0,episode_reward_mean=15.22,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4770029847126984, 'mean_inference_ms': 6.765207145820499, 'mean_action_processing_ms': 0.3346017472584832, 'mean_env_wait_ms': 0.20181125818629325, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=24000,timers={'sample_time_ms': 1772.569, 'sample_throughput': 2256.612, 'learn_time_ms': 8902.414, 'learn_throughput': 449.316},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.21474349575164992, 'policy_loss': -0.2205519855547792, 'vf_loss': 82.37236516903609, 'vf_explained_var': 0.08511002, 'kl': 0.01807348979398226, 'entropy': 0.43728122191551405, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 24000, 'num_agent_steps_sampled': 24000, 'num_steps_trained': 24000},perf={'cpu_util_percent': 69.38125, 'ram_util_percent': 35.33125} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=55.0,episode_reward_min=-9.0,episode_reward_mean=25.06,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4807172410552918, 'mean_inference_ms': 6.755210633601357, 'mean_action_processing_ms': 0.3330695873452396, 'mean_env_wait_ms': 0.20479385717664367, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=28000,timers={'sample_time_ms': 1744.683, 'sample_throughput': 2292.68, 'learn_time_ms': 8929.899, 'learn_throughput': 447.933},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.23007085050145784, 'policy_loss': -0.23498127494867033, 'vf_loss': 166.31720459766876, 'vf_explained_var': 0.11487528, 'kl': 0.012137481632332007, 'entropy': 0.393991408439783, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000},perf={'cpu_util_percent': 63.16, 'ram_util_percent': 35.339999999999996} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=55.0,episode_reward_min=-3.0,episode_reward_mean=34.92,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.458181524014637, 'mean_inference_ms': 6.687810939327307, 'mean_action_processing_ms': 0.3303239961215765, 'mean_env_wait_ms': 0.20484054084749087, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=32000,timers={'sample_time_ms': 1720.918, 'sample_throughput': 2324.341, 'learn_time_ms': 8925.472, 'learn_throughput': 448.156},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.24697113849031618, 'policy_loss': -0.25117671145842624, 'vf_loss': 225.1361138759515, 'vf_explained_var': 0.15071493, 'kl': 0.007780143328440877, 'entropy': 0.37983329326678544, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 32000, 'num_agent_steps_sampled': 32000, 'num_steps_trained': 32000},perf={'cpu_util_percent': 62.566666666666656, 'ram_util_percent': 35.313333333333325} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=65.0,episode_reward_min=17.0,episode_reward_mean=40.12,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4397916751862412, 'mean_inference_ms': 6.630151348596794, 'mean_action_processing_ms': 0.3281204268152734, 'mean_env_wait_ms': 0.2041231593991664, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=36000,timers={'sample_time_ms': 1718.585, 'sample_throughput': 2327.496, 'learn_time_ms': 8913.132, 'learn_throughput': 448.776},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.23940366745377198, 'policy_loss': -0.24370483923942232, 'vf_loss': 182.19507970565405, 'vf_explained_var': 0.1991841, 'kl': 0.009509079444866914, 'entropy': 0.37350391959532714, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 36000, 'num_agent_steps_sampled': 36000, 'num_steps_trained': 36000},perf={'cpu_util_percent': 63.50666666666666, 'ram_util_percent': 35.32666666666667} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=77.0,episode_reward_min=25.0,episode_reward_mean=48.28,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4269652831962105, 'mean_inference_ms': 6.581943211933481, 'mean_action_processing_ms': 0.326845058346101, 'mean_env_wait_ms': 0.20327411127185122, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=40000,timers={'sample_time_ms': 1700.818, 'sample_throughput': 2351.81, 'learn_time_ms': 8923.607, 'learn_throughput': 448.249},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.2583254535133258, 'policy_loss': -0.2636135093246897, 'vf_loss': 360.50916896722254, 'vf_explained_var': 0.196251, 'kl': 0.006742365825443695, 'entropy': 0.33973846527246326, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000},perf={'cpu_util_percent': 64.73333333333332, 'ram_util_percent': 35.29333333333333} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=83.0,episode_reward_min=25.0,episode_reward_mean=55.16,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4266409014773311, 'mean_inference_ms': 6.587923741224315, 'mean_action_processing_ms': 0.32737878569161905, 'mean_env_wait_ms': 0.2031197030249075, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=44000,timers={'sample_time_ms': 1741.321, 'sample_throughput': 2297.106, 'learn_time_ms': 8993.161, 'learn_throughput': 444.782},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.2531465314901792, 'policy_loss': -0.2584335448172612, 'vf_loss': 317.0672392233824, 'vf_explained_var': 0.23115143, 'kl': 0.008031922416427197, 'entropy': 0.29322903660627514, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 44000, 'num_agent_steps_sampled': 44000, 'num_steps_trained': 44000},perf={'cpu_util_percent': 66.49375, 'ram_util_percent': 35.38125} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=83.0,episode_reward_min=43.0,episode_reward_mean=57.98,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4333202893379502, 'mean_inference_ms': 6.617110137732623, 'mean_action_processing_ms': 0.3284057048515823, 'mean_env_wait_ms': 0.2032385183594697, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=48000,timers={'sample_time_ms': 1751.142, 'sample_throughput': 2284.224, 'learn_time_ms': 8930.167, 'learn_throughput': 447.92},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 5e-05, 'total_loss': -0.06860341639681296, 'policy_loss': -0.0739517378987688, 'vf_loss': 210.52734999223188, 'vf_explained_var': 0.33569148, 'kl': 0.011758392253382639, 'entropy': 0.28446670147505676, 'entropy_coeff': 0.001}}}), 'num_steps_sampled': 48000, 'num_agent_steps_sampled': 48000, 'num_steps_trained': 48000},perf={'cpu_util_percent': 65.03571428571428, 'ram_util_percent': 35.35714285714285} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=83.0,episode_reward_min=43.0,episode_reward_mean=57.14,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4353122320891476, 'mean_inference_ms': 6.628974315376631, 'mean_action_processing_ms': 0.32862090486932133, 'mean_env_wait_ms': 0.20325922601814736, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=52000,timers={'sample_time_ms': 1697.63, 'sample_throughput': 2356.227, 'learn_time_ms': 8775.594, 'learn_throughput': 455.81},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 5e-05, 'total_loss': -0.05759186107157306, 'policy_loss': -0.06077779883123709, 'vf_loss': 171.31165140325373, 'vf_explained_var': 0.38437533, 'kl': 0.0058139006000463705, 'entropy': 0.2713438945286202, 'entropy_coeff': 0.001}}}), 'num_steps_sampled': 52000, 'num_agent_steps_sampled': 52000, 'num_steps_trained': 52000},perf={'cpu_util_percent': 61.05384615384616, 'ram_util_percent': 35.33076923076923} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=81.0,episode_reward_min=43.0,episode_reward_mean=58.38,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4275658640236648, 'mean_inference_ms': 6.6003401160372555, 'mean_action_processing_ms': 0.3275775725563897, 'mean_env_wait_ms': 0.20224863808747132, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=56000,timers={'sample_time_ms': 1695.043, 'sample_throughput': 2359.822, 'learn_time_ms': 8614.459, 'learn_throughput': 464.336},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 5e-05, 'total_loss': -0.06711328873467265, 'policy_loss': -0.07060255733967731, 'vf_loss': 192.79995958732837, 'vf_explained_var': 0.3763244, 'kl': 0.006031345745379274, 'entropy': 0.2481345213723905, 'entropy_coeff': 0.001}}}), 'num_steps_sampled': 56000, 'num_agent_steps_sampled': 56000, 'num_steps_trained': 56000},perf={'cpu_util_percent': 58.73076923076923, 'ram_util_percent': 35.26153846153846} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=83.0,episode_reward_min=43.0,episode_reward_mean=62.58,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4189268668957973, 'mean_inference_ms': 6.574621250977603, 'mean_action_processing_ms': 0.32682730246982644, 'mean_env_wait_ms': 0.20132242402280792, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=60000,timers={'sample_time_ms': 1675.815, 'sample_throughput': 2386.899, 'learn_time_ms': 8610.547, 'learn_throughput': 464.547},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.0615456658296096, 'policy_loss': -0.06544008897617459, 'vf_loss': 237.96452351105518, 'vf_explained_var': 0.3435052, 'kl': 0.005866558968614883, 'entropy': 0.2451880803475013, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 60000, 'num_agent_steps_sampled': 60000, 'num_steps_trained': 60000},perf={'cpu_util_percent': 64.55333333333334, 'ram_util_percent': 35.28} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=83.0,episode_reward_min=47.0,episode_reward_mean=67.42,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4110745683824877, 'mean_inference_ms': 6.569229904107318, 'mean_action_processing_ms': 0.32606453537700425, 'mean_env_wait_ms': 0.200273801066659, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=64000,timers={'sample_time_ms': 1663.617, 'sample_throughput': 2404.4, 'learn_time_ms': 8533.881, 'learn_throughput': 468.72},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.02684989074865977, 'policy_loss': -0.030516286882070396, 'vf_loss': 214.83422069060498, 'vf_explained_var': 0.37533453, 'kl': 0.005839896824163122, 'entropy': 0.23391465613475212, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 64000, 'num_agent_steps_sampled': 64000, 'num_steps_trained': 64000},perf={'cpu_util_percent': 60.57333333333334, 'ram_util_percent': 35.34} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=89.0,episode_reward_min=57.0,episode_reward_mean=71.68,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.4040029582396056, 'mean_inference_ms': 6.559318131019584, 'mean_action_processing_ms': 0.32510732518111185, 'mean_env_wait_ms': 0.1991124960342531, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=68000,timers={'sample_time_ms': 1661.484, 'sample_throughput': 2407.486, 'learn_time_ms': 8488.023, 'learn_throughput': 471.252},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.022513885146532304, 'policy_loss': -0.026445078544127636, 'vf_loss': 218.26651529165414, 'vf_explained_var': 0.39457232, 'kl': 0.006590758050338198, 'entropy': 0.22869519660106072, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 68000, 'num_agent_steps_sampled': 68000, 'num_steps_trained': 68000},perf={'cpu_util_percent': 60.98571428571429, 'ram_util_percent': 35.22142857142857} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=89.0,episode_reward_min=61.0,episode_reward_mean=73.84,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3967287167581321, 'mean_inference_ms': 6.533361779317611, 'mean_action_processing_ms': 0.32386943592627915, 'mean_env_wait_ms': 0.1977393963358532, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=72000,timers={'sample_time_ms': 1659.322, 'sample_throughput': 2410.623, 'learn_time_ms': 8466.16, 'learn_throughput': 472.469},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000016, 'cur_lr': 4.999999999999999e-05, 'total_loss': -0.0007730551684896151, 'policy_loss': -0.003831687192313182, 'vf_loss': 202.08311286339392, 'vf_explained_var': 0.41089502, 'kl': 0.004186680683722863, 'entropy': 0.21820225432897225, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 72000, 'num_agent_steps_sampled': 72000, 'num_steps_trained': 72000},perf={'cpu_util_percent': 61.60666666666667, 'ram_util_percent': 35.2} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=89.0,episode_reward_min=61.0,episode_reward_mean=74.6,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3899317344484063, 'mean_inference_ms': 6.502977999786173, 'mean_action_processing_ms': 0.32278206824592715, 'mean_env_wait_ms': 0.19646282820846153, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=76000,timers={'sample_time_ms': 1645.661, 'sample_throughput': 2430.634, 'learn_time_ms': 8536.382, 'learn_throughput': 468.583},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.15000000000000008, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.003303096987880193, 'policy_loss': -0.002040622469324332, 'vf_loss': 175.22357236422025, 'vf_explained_var': 0.39981464, 'kl': 0.025665453396355495, 'entropy': 0.258332732014167, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 76000, 'num_agent_steps_sampled': 76000, 'num_steps_trained': 76000},perf={'cpu_util_percent': 68.58125, 'ram_util_percent': 35.33125} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=87.0,episode_reward_min=51.0,episode_reward_mean=74.94,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3842451999007563, 'mean_inference_ms': 6.482094675470514, 'mean_action_processing_ms': 0.3218904268328415, 'mean_env_wait_ms': 0.19541455254599896, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=80000,timers={'sample_time_ms': 1652.877, 'sample_throughput': 2420.023, 'learn_time_ms': 8578.127, 'learn_throughput': 466.302},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.22499999999999995, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.01601493241599737, 'policy_loss': 0.013240917729070554, 'vf_loss': 177.41213695819562, 'vf_explained_var': 0.39894548, 'kl': 0.005448749820248057, 'entropy': 0.22607445296568748, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 80000, 'num_agent_steps_sampled': 80000, 'num_steps_trained': 80000},perf={'cpu_util_percent': 68.83333333333334, 'ram_util_percent': 35.43999999999999} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.7/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=87.0,episode_reward_min=51.0,episode_reward_mean=74.28,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.379268833967224, 'mean_inference_ms': 6.462322835976299, 'mean_action_processing_ms': 0.3209918891121432, 'mean_env_wait_ms': 0.1943646828732694, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=84000,timers={'sample_time_ms': 1612.222, 'sample_throughput': 2481.048, 'learn_time_ms': 8586.871, 'learn_throughput': 465.827},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.22499999999999995, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.04176911766616962, 'policy_loss': 0.0387442176325772, 'vf_loss': 181.1648696508163, 'vf_explained_var': 0.37815127, 'kl': 0.00635280501909363, 'entropy': 0.21612822933074755, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 84000, 'num_agent_steps_sampled': 84000, 'num_steps_trained': 84000},perf={'cpu_util_percent': 65.56875, 'ram_util_percent': 35.4} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=91.0,episode_reward_min=51.0,episode_reward_mean=73.86,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3760599152291924, 'mean_inference_ms': 6.451622385486343, 'mean_action_processing_ms': 0.3206388379201103, 'mean_env_wait_ms': 0.1938132379008821, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=88000,timers={'sample_time_ms': 1602.661, 'sample_throughput': 2495.849, 'learn_time_ms': 8725.524, 'learn_throughput': 458.425},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.22499999999999995, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.06120070296888932, 'policy_loss': 0.05821109472368008, 'vf_loss': 180.77151919633914, 'vf_explained_var': 0.36623883, 'kl': 0.006167442093077951, 'entropy': 0.2057873736589383, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 88000, 'num_agent_steps_sampled': 88000, 'num_steps_trained': 88000},perf={'cpu_util_percent': 67.24666666666667, 'ram_util_percent': 35.406666666666666} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=91.0,episode_reward_min=57.0,episode_reward_mean=75.9,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3740466644696558, 'mean_inference_ms': 6.43692825211074, 'mean_action_processing_ms': 0.32024688799984596, 'mean_env_wait_ms': 0.19337348047606714, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=92000,timers={'sample_time_ms': 1594.453, 'sample_throughput': 2508.698, 'learn_time_ms': 8951.401, 'learn_throughput': 446.857},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.22499999999999995, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.06321624181687067, 'policy_loss': 0.060363077582457125, 'vf_loss': 198.18274590907953, 'vf_explained_var': 0.40250644, 'kl': 0.004732099110977008, 'entropy': 0.19338811208040285, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 92000, 'num_agent_steps_sampled': 92000, 'num_steps_trained': 92000},perf={'cpu_util_percent': 69.01764705882353, 'ram_util_percent': 35.45882352941176} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/3.45 GiB heap, 0.0/1.73 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/e1e8a549f676d3104ff30c678c89aab1/PPO
Number of trials: 1/1 (1 RUNNING)


Trial PPO_RepeatAfterMeEnv_b1753_00000 reported episode_reward_max=93.0,episode_reward_min=61.0,episode_reward_mean=78.32,episode_len_mean=99.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 1.3710774800365322, 'mean_inference_ms': 6.41723853348667, 'mean_action_processing_ms': 0.31991860605322936, 'mean_env_wait_ms': 0.19288660378864203, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=96000,timers={'sample_time_ms': 1592.71, 'sample_throughput': 2511.442, 'learn_time_ms': 9153.187, 'learn_throughput': 437.006},info={'learner': defaultdict(<class 'dict'>, {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.11249999999999998, 'cur_lr': 4.999999999999999e-05, 'total_loss': 0.07552991145982957, 'policy_loss': 0.07303355801372956, 'vf_loss': 200.515380468124, 'vf_explained_var': 0.4039828, 'kl': 0.006066449711887309, 'entropy': 0.19127412140369415, 'entropy_coeff': 0.0010000000000000002}}}), 'num_steps_sampled': 96000, 'num_agent_steps_sampled': 96000, 'num_steps_trained': 96000},perf={'cpu_util_percent': 68.33999999999999, 'ram_util_percent': 35.38666666666666} with parameters={'env': 'RepeatAfterMeEnv', 'env_config': {'repeat_delay': 2}, 'gamma': 0.99, 'num_gpus': 0, 'num_workers': 0, 'num_envs_per_worker': 20, 'entropy_coeff': 0.001, 'num_sgd_iter': 10, 'vf_loss_coeff': 1e-05, 'model': {'use_attention': True, 'max_seq_len': 50, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_memory_inference': 100, 'attention_memory_training': 50, 'attention_num_heads': 2, 'attention_head_dim': 32, 'attention_position_wise_mlp_dim': 32}, 'framework': 'torch'}.-- Test timed out at 2021-04-09 12:30:31 UTC --
[2m[36m(pid=6187)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=6187)[0m Instructions for updating:
[2m[36m(pid=6187)[0m non-resource variables are not supported in the long term
[2m[36m(pid=6187)[0m 2021-04-09 12:25:42,828	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=6187)[0m /home/ray/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
[2m[36m(pid=6187)[0m   return torch._C._cuda_getDeviceCount() > 0
[2m[36m(pid=6187)[0m 2021-04-09 12:25:43,016	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=6187)[0m 2021-04-09 12:25:44,554	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=6187)[0m 2021-04-09 12:25:44,554	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
2021-04-09 12:30:32,028	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 12:30:32,098	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
2021-04-09 12:30:32,099	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 12:30:32,100	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
================================================================================
(05:33:30) [32mINFO: [0mElapsed time: 1891.611s, Critical Path: 922.25s
(05:33:30) [32mINFO: [0m32 processes: 32 local.
(05:33:30) [32mINFO:[0m Build completed, 1 test FAILED, 42 total actions
//rllib:examples/autoregressive_action_dist_tf                           [0m[32mPASSED[0m in 65.1s
//rllib:examples/autoregressive_action_dist_torch                        [0m[32mPASSED[0m in 238.3s
//rllib:examples/batch_norm_model_ddpg_torch                             [0m[32mPASSED[0m in 52.7s
//rllib:examples/batch_norm_model_dqn_tf                                 [0m[32mPASSED[0m in 108.0s
//rllib:examples/batch_norm_model_dqn_torch                              [0m[32mPASSED[0m in 600.1s
//rllib:examples/batch_norm_model_ppo_tf                                 [0m[32mPASSED[0m in 77.5s
//rllib:examples/batch_norm_model_ppo_torch                              [0m[32mPASSED[0m in 123.2s
//rllib:examples/attention_net_tf                                         [0m[35mFLAKY[0m, failed in 2 out of 3 in 315.0s
  Stats over 3 runs: max = 315.0s, min = 291.8s, avg = 307.3s, dev = 11.0s
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_tf/test_attempts/attempt_1.log
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_tf/test_attempts/attempt_2.log
//rllib:examples/attention_net_torch                                      [0m[35mFLAKY[0m, failed in 2 out of 3 in 315.0s
  Stats over 3 runs: max = 315.0s, min = 163.3s, avg = 264.4s, dev = 71.5s
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_torch/test_attempts/attempt_1.log
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/attention_net_torch/test_attempts/attempt_2.log
//rllib:examples/batch_norm_model_ddpg_tf                               [0m[31m[1mTIMEOUT[0m in 3 out of 3 in 75.0s
  Stats over 3 runs: max = 75.0s, min = 75.0s, avg = 75.0s, dev = 0.0s
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/batch_norm_model_ddpg_tf/test.log
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/batch_norm_model_ddpg_tf/test_attempts/attempt_1.log
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/batch_norm_model_ddpg_tf/test_attempts/attempt_2.log

Executed 10 out of 10 tests: 9 tests pass and [0m[31m[1m1 fails locally[0m.
(05:33:30) [32mINFO: [0mBuild Event Protocol files produced successfully.
(05:33:30) [32mINFO:[0m Build completed, 1 test FAILED, 42 total actions
[0m(05:33:30) [32mINFO: [0mCurrent date is 2021-04-09
(05:33:30) [32mLoading:[0m 
(05:33:31) [32mLoading:[0m 0 packages loaded
(05:33:31) [33mDEBUG: [0m/home/ray/release-automation-rllib_unit_gpu_tests/ray/bazel/ray_deps_setup.bzl:63:14: No implicit mirrors used because urls were explicitly provided
(05:33:31) [32mAnalyzing:[0m 36 targets (0 packages loaded, 0 targets configured)
(05:33:31) [32mINFO: [0mAnalyzed 36 targets (0 packages loaded, 54 targets configured).
(05:33:31) [32mINFO: [0mFound 36 test targets...
(05:33:31) [32m[0 / 1][0m [Prepa] BazelWorkspaceStatusAction stable-status.txt
(05:34:01) [32m[10 / 12][0m Testing //rllib:examples/cartpole_lstm_impala_torch; 29s local ... (2 actions running)
(05:34:31) [32m[10 / 12][0m Testing //rllib:examples/cartpole_lstm_impala_torch; 59s local ... (2 actions running)
(05:35:01) [32m[10 / 12][0m Testing //rllib:examples/cartpole_lstm_impala_torch; 89s local ... (2 actions running)
(05:35:16) [32m[10 / 12][0m Testing //rllib:examples/cartpole_lstm_impala_torch; 104s local ... (2 actions running)
(05:35:47) [32m[10 / 12][0m Testing //rllib:examples/cartpole_lstm_impala_torch; 136s local ... (2 actions running)
(05:36:28) [32m[10 / 12][0m Testing //rllib:examples/cartpole_lstm_impala_torch; 177s local ... (2 actions running)
(05:36:55) [32m[18 / 20][0m 2 / 36 tests;[0m Testing //rllib:examples/custom_fast_model_tf; 23s local ... (2 actions running)
(05:37:26) [32m[26 / 28][0m 4 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf; 9s local ... (2 actions running)
(05:38:01) [32m[26 / 28][0m 4 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf; 45s local ... (2 actions running)
(05:38:42) [32m[26 / 28][0m 4 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf; 85s local ... (2 actions running)
(05:39:30) [32m[26 / 28][0m 4 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf; 134s local ... (2 actions running)
(05:41:09) [32m[30 / 32][0m 5 / 36 tests;[0m Testing //rllib:examples/centralized_critic_torch; 227s local ... (2 actions running)
(05:42:18) [32m[30 / 32][0m 5 / 36 tests;[0m Testing //rllib:examples/centralized_critic_torch; 296s local ... (2 actions running)
(05:44:32) [32m[30 / 32][0m 5 / 36 tests;[0m Testing //rllib:examples/centralized_critic_torch; 430s local ... (2 actions running)
(05:46:35) [32m[34 / 36][0m 6 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 424s local ... (2 actions running)
(05:48:33) [32m[34 / 36][0m 6 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 542s local ... (2 actions running)
(05:50:49) [32m[38 / 40][0m 7 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 678s local ... (2 actions running)
(05:53:24) [32m[42 / 44][0m 8 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 833s local ... (2 actions running)
(05:56:24) [32m[50 / 52][0m 10 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 97s local ... (2 actions running)
(05:59:50) [32m[62 / 64][0m 13 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 303s local ... (2 actions running)
(06:03:46) [32m[74 / 76][0m 16 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 540s local ... (2 actions running)
(06:08:19) [32m[79 / 81][0m 18 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 813s local ... (2 actions running)
(06:13:32) [32m[95 / 97][0m 22 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 211s local ... (2 actions running)
(06:19:32) [32m[127 / 129][0m 30 / 36 tests;[0m Testing //rllib:examples/cartpole_lstm_ppo_tf2; 571s local ... (2 actions running)

[31m[1mTIMEOUT: [0m//rllib:examples/cartpole_lstm_ppo_tf2 (Summary)
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/cartpole_lstm_ppo_tf2/test.log
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/cartpole_lstm_ppo_tf2/test_attempts/attempt_1.log
      /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/cartpole_lstm_ppo_tf2/test_attempts/attempt_2.log
(06:25:16) [32mINFO: [0mFrom Testing //rllib:examples/cartpole_lstm_ppo_tf2:
==================== Test output for //rllib:examples/cartpole_lstm_ppo_tf2:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:39:40,857	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 3.3/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/2.87 GiB heap, 0.0/1.44 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9c0c18e20e172baeb1a71b6ba1ca6ed8/PPO
Number of trials: 1/1 (1 RUNNING)


[2m[36m(pid=8847)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=8846)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=8914)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=8846)[0m Model: "model_1"
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=8846)[0m ==================================================================================================
[2m[36m(pid=8846)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     
[2m[36m(pid=8846)[0m                                                                  h[0][0]                          
[2m[36m(pid=8846)[0m                                                                  c[0][0]                          
[2m[36m(pid=8846)[0m                                                                  tf.sequence_mask[0][0]           
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m logits (Dense)                  (None, None, 2)      514         lstm[0][0]                       
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8846)[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       
[2m[36m(pid=8846)[0m ==================================================================================================
[2m[36m(pid=8846)[0m Total params: 526,083
[2m[36m(pid=8846)[0m Trainable params: 526,083
[2m[36m(pid=8846)[0m Non-trainable params: 0
[2m[36m(pid=8846)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m Model: "model_1"
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=8914)[0m ==================================================================================================
[2m[36m(pid=8914)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     
[2m[36m(pid=8914)[0m                                                                  h[0][0]                          
[2m[36m(pid=8914)[0m                                                                  c[0][0]                          
[2m[36m(pid=8914)[0m                                                                  tf.sequence_mask[0][0]           
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m logits (Dense)                  (None, None, 2)      514         lstm[0][0]                       
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8914)[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       
[2m[36m(pid=8914)[0m ==================================================================================================
[2m[36m(pid=8914)[0m Total params: 526,083
[2m[36m(pid=8914)[0m Trainable params: 526,083
[2m[36m(pid=8914)[0m Non-trainable params: 0
[2m[36m(pid=8914)[0m __________________________________________________________________________________________________
[2m[36m(pid=8847)[0m Model: "model_1"
[2m[36m(pid=8847)[0m __________________________________________________________________________________________________
[2m[36m(pid=8847)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=8847)[0m ==================================================================================================
[2m[36m(pid=8847)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=8847)[0m __________________________________________________________________________________________________
[2m[36m(pid=8847)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=8847)[0m __________________________________________________________________________________________________
[2m[36m(pid=8847)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=8847)[0m __________________________________________________________________________________________________
[2m[36m(pid=8847)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=8847)[0m __________________________________________________________________________________________________
[2m[36m(pid=8847)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     -- Test timed out at 2021-04-09 12:54:31 UTC --
[2m[36m(pid=8847)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=8847)[0m Instructions for updating:
[2m[36m(pid=8847)[0m non-resource variables are not supported in the long term
[2m[36m(pid=8847)[0m 2021-04-09 12:39:51,837	INFO trainer.py:666 -- Executing eagerly, with eager_tracing=True
[2m[36m(pid=8847)[0m 2021-04-09 12:39:51,837	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=8846)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=8846)[0m Instructions for updating:
[2m[36m(pid=8846)[0m non-resource variables are not supported in the long term
[2m[36m(pid=8914)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=8914)[0m Instructions for updating:
[2m[36m(pid=8914)[0m non-resource variables are not supported in the long term
[2m[36m(pid=8846)[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
[2m[36m(pid=8846)[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[2m[36m(pid=8914)[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
[2m[36m(pid=8914)[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[2m[36m(pid=8847)[0m 2021-04-09 12:40:07,101	INFO trainable.py:104 -- Trainable.setup took 15.265 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=8847)[0m 2021-04-09 12:40:07,101	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=8847)[0m 2021-04-09 12:41:00,789	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=8847)[0m 2021-04-09 12:41:00,789	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
2021-04-09 12:54:31,038	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 12:54:31,078	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 12:54:31,080	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
2021-04-09 12:54:31,080	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
================================================================================
==================== Test output for //rllib:examples/cartpole_lstm_ppo_tf2:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 12:54:54,831	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8266[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 2.6/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9c0c18e20e172baeb1a71b6ba1ca6ed8/PPO
Number of trials: 1/1 (1 RUNNING)


[2m[36m(pid=10591)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=10590)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=10803)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=10590)[0m Model: "model_1"
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=10590)[0m ==================================================================================================
[2m[36m(pid=10590)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     
[2m[36m(pid=10590)[0m                                                                  h[0][0]                          
[2m[36m(pid=10590)[0m                                                                  c[0][0]                          
[2m[36m(pid=10590)[0m                                                                  tf.sequence_mask[0][0]           
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m logits (Dense)                  (None, None, 2)      514         lstm[0][0]                       
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10590)[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       
[2m[36m(pid=10590)[0m ==================================================================================================
[2m[36m(pid=10590)[0m Total params: 526,083
[2m[36m(pid=10590)[0m Trainable params: 526,083
[2m[36m(pid=10590)[0m Non-trainable params: 0
[2m[36m(pid=10590)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m Model: "model_1"
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=10803)[0m ==================================================================================================
[2m[36m(pid=10803)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     
[2m[36m(pid=10803)[0m                                                                  h[0][0]                          
[2m[36m(pid=10803)[0m                                                                  c[0][0]                          
[2m[36m(pid=10803)[0m                                                                  tf.sequence_mask[0][0]           
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m logits (Dense)                  (None, None, 2)      514         lstm[0][0]                       
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10803)[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       
[2m[36m(pid=10803)[0m ==================================================================================================
[2m[36m(pid=10803)[0m Total params: 526,083
[2m[36m(pid=10803)[0m Trainable params: 526,083
[2m[36m(pid=10803)[0m Non-trainable params: 0
[2m[36m(pid=10803)[0m __________________________________________________________________________________________________
[2m[36m(pid=10591)[0m Model: "model_1"
[2m[36m(pid=10591)[0m __________________________________________________________________________________________________
[2m[36m(pid=10591)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=10591)[0m ==================================================================================================
[2m[36m(pid=10591)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=10591)[0m __________________________________________________________________________________________________
[2m[36m(pid=10591)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=10591)[0m __________________________________________________________________________________________________
[2m[36m(pid=10591)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=10591)[0m __________________________________________________________________________________________________
[2m[36m(pid=10591)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=10591)[0m __________________________________________________________________________________________________
[2m[36m(pid=10591)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     -- Test timed out at 2021-04-09 13:09:46 UTC --
[2m[36m(pid=10591)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=10591)[0m Instructions for updating:
[2m[36m(pid=10591)[0m non-resource variables are not supported in the long term
[2m[36m(pid=10591)[0m 2021-04-09 12:55:05,090	INFO trainer.py:666 -- Executing eagerly, with eager_tracing=True
[2m[36m(pid=10591)[0m 2021-04-09 12:55:05,091	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=10590)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=10590)[0m Instructions for updating:
[2m[36m(pid=10590)[0m non-resource variables are not supported in the long term
[2m[36m(pid=10803)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=10803)[0m Instructions for updating:
[2m[36m(pid=10803)[0m non-resource variables are not supported in the long term
[2m[36m(pid=10590)[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
[2m[36m(pid=10590)[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[2m[36m(pid=10803)[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
[2m[36m(pid=10803)[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[2m[36m(pid=10591)[0m 2021-04-09 12:55:19,025	INFO trainable.py:104 -- Trainable.setup took 13.936 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=10591)[0m 2021-04-09 12:55:19,026	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=10591)[0m 2021-04-09 12:56:39,129	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=10591)[0m 2021-04-09 12:56:39,133	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
2021-04-09 13:09:46,110	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 13:09:46,131	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
2021-04-09 13:09:46,131	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 13:09:46,132	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
================================================================================
==================== Test output for //rllib:examples/cartpole_lstm_ppo_tf2:
WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-04-09 13:10:09,520	INFO services.py:1269 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8267[39m[22m
Loading environment football failed: No module named 'gfootball'
== Status ==
Memory usage on this node: 3.0/7.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/3.2 GiB heap, 0.0/1.6 GiB objects
Result logdir: /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/_tmp/9c0c18e20e172baeb1a71b6ba1ca6ed8/PPO
Number of trials: 1/1 (1 PENDING)


[2m[36m(pid=14323)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=14325)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=14442)[0m Loading environment football failed: No module named 'gfootball'
[2m[36m(pid=14325)[0m Model: "model_1"
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=14325)[0m ==================================================================================================
[2m[36m(pid=14325)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     
[2m[36m(pid=14325)[0m                                                                  h[0][0]                          
[2m[36m(pid=14325)[0m                                                                  c[0][0]                          
[2m[36m(pid=14325)[0m                                                                  tf.sequence_mask[0][0]           
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m logits (Dense)                  (None, None, 2)      514         lstm[0][0]                       
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14325)[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       
[2m[36m(pid=14325)[0m ==================================================================================================
[2m[36m(pid=14325)[0m Total params: 526,083
[2m[36m(pid=14325)[0m Trainable params: 526,083
[2m[36m(pid=14325)[0m Non-trainable params: 0
[2m[36m(pid=14325)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m Model: "model_1"
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=14442)[0m ==================================================================================================
[2m[36m(pid=14442)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     
[2m[36m(pid=14442)[0m                                                                  h[0][0]                          
[2m[36m(pid=14442)[0m                                                                  c[0][0]                          
[2m[36m(pid=14442)[0m                                                                  tf.sequence_mask[0][0]           
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m logits (Dense)                  (None, None, 2)      514         lstm[0][0]                       
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14442)[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       
[2m[36m(pid=14442)[0m ==================================================================================================
[2m[36m(pid=14442)[0m Total params: 526,083
[2m[36m(pid=14442)[0m Trainable params: 526,083
[2m[36m(pid=14442)[0m Non-trainable params: 0
[2m[36m(pid=14442)[0m __________________________________________________________________________________________________
[2m[36m(pid=14323)[0m Model: "model_1"
[2m[36m(pid=14323)[0m __________________________________________________________________________________________________
[2m[36m(pid=14323)[0m Layer (type)                    Output Shape         Param #     Connected to                     
[2m[36m(pid=14323)[0m ==================================================================================================
[2m[36m(pid=14323)[0m seq_in (InputLayer)             [(None,)]            0                                            
[2m[36m(pid=14323)[0m __________________________________________________________________________________________________
[2m[36m(pid=14323)[0m inputs (InputLayer)             [(None, None, 256)]  0                                            
[2m[36m(pid=14323)[0m __________________________________________________________________________________________________
[2m[36m(pid=14323)[0m h (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=14323)[0m __________________________________________________________________________________________________
[2m[36m(pid=14323)[0m c (InputLayer)                  [(None, 256)]        0                                            
[2m[36m(pid=14323)[0m __________________________________________________________________________________________________
[2m[36m(pid=14323)[0m tf.sequence_mask (TFOpLambda)   (None, None)         0           seq_in[0][0]                     -- Test timed out at 2021-04-09 13:25:01 UTC --
[2m[36m(pid=14323)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=14323)[0m Instructions for updating:
[2m[36m(pid=14323)[0m non-resource variables are not supported in the long term
[2m[36m(pid=14323)[0m 2021-04-09 13:10:18,521	INFO trainer.py:666 -- Executing eagerly, with eager_tracing=True
[2m[36m(pid=14323)[0m 2021-04-09 13:10:18,521	INFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=14325)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=14325)[0m Instructions for updating:
[2m[36m(pid=14325)[0m non-resource variables are not supported in the long term
[2m[36m(pid=14442)[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=14442)[0m Instructions for updating:
[2m[36m(pid=14442)[0m non-resource variables are not supported in the long term
[2m[36m(pid=14325)[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
[2m[36m(pid=14325)[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[2m[36m(pid=14442)[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
[2m[36m(pid=14442)[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[2m[36m(pid=14323)[0m 2021-04-09 13:10:34,005	INFO trainable.py:104 -- Trainable.setup took 15.488 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=14323)[0m 2021-04-09 13:10:34,005	WARNING util.py:53 -- Install gputil for GPU system monitoring.
[2m[36m(pid=14323)[0m 2021-04-09 13:11:31,560	WARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!
[2m[36m(pid=14323)[0m 2021-04-09 13:11:31,560	WARNING sgd.py:69 -- Not time-shuffling RNN data for SGD.
2021-04-09 13:25:01,157	WARNING worker.py:1085 -- The autoscaler failed with the following error:
Terminated with signal 15
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 376, in <module>
    monitor.run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 284, in run
    self._run()
  File "/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/monitor.py", line 202, in _run
    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)

2021-04-09 13:25:01,234	ERROR worker.py:928 -- print_logs: Connection closed by server.
2021-04-09 13:25:01,241	ERROR worker.py:1087 -- listen_error_messages_raylet: Connection closed by server.
2021-04-09 13:25:01,242	ERROR import_thread.py:88 -- ImportThread: Connection closed by server.
================================================================================
(06:25:31) [32mINFO: [0mElapsed time: 3120.784s, Critical Path: 2745.20s
(06:25:31) [32mINFO: [0m76 processes: 76 local.
(06:25:31) [32mINFO:[0m Build completed, 1 test FAILED, 145 total actions
//rllib:examples/cartpole_lstm_impala_tf                                 [0m[32mPASSED[0m in 111.9s
  [0m[35mWARNING: [0m//rllib:examples/cartpole_lstm_impala_tf: Test execution time (111.9s excluding execution overhead) outside of range for LONG tests. Consider setting timeout="moderate" or size="medium".
//rllib:examples/cartpole_lstm_impala_torch                              [0m[32mPASSED[0m in 185.7s
//rllib:examples/cartpole_lstm_ppo_tf                                    [0m[32mPASSED[0m in 134.5s
  [0m[35mWARNING: [0m//rllib:examples/cartpole_lstm_ppo_tf: Test execution time (134.5s excluding execution overhead) outside of range for LONG tests. Consider setting timeout="moderate" or size="medium".
//rllib:examples/cartpole_lstm_ppo_tf_with_prev_a_and_r                  [0m[32mPASSED[0m in 180.0s
//rllib:examples/cartpole_lstm_ppo_torch                                 [0m[32mPASSED[0m in 87.4s
  [0m[35mWARNING: [0m//rllib:examples/cartpole_lstm_ppo_torch: Test execution time (87.4s excluding execution overhead) outside of range for LONG tests. Consider setting timeout="moderate" or size="medium".
//rllib:examples/centralized_critic_2_tf                                 [0m[32mPASSED[0m in 82.1s
//rllib:examples/centralized_critic_2_torch                              [0m[32mPASSED[0m in 126.4s
//rllib:examples/centralized_critic_tf                                   [0m[32mPASSED[0m in 241.0s
//rllib:examples/centralized_critic_torch                                [0m[32mPASSED[0m in 514.6s
//rllib:examples/checkpoint_by_custom_criteria                           [0m[32mPASSED[0m in 183.2s
//rllib:examples/complex_struct_space_tf                                 [0m[32mPASSED[0m in 22.7s
  [0m[35mWARNING: [0m//rllib:examples/complex_struct_space_tf: Test execution time (22.7s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout="short" or size="small".
//rllib:examples/complex_struct_space_tf_eager                           [0m[32mPASSED[0m in 21.8s
  [0m[35mWARNING: [0m//rllib:examples/complex_struct_space_tf_eager: Test execution time (21.8s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout="short" or size="small".
//rllib:examples/complex_struct_space_torch                              [0m[32mPASSED[0m in 26.4s
  [0m[35mWARNING: [0m//rllib:examples/complex_struct_space_torch: Test execution time (26.4s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout="short" or size="small".
//rllib:examples/custom_env_tf                                           [0m[32mPASSED[0m in 173.8s
//rllib:examples/custom_env_torch                                        [0m[32mPASSED[0m in 234.3s
//rllib:examples/custom_eval_tf                                          [0m[32mPASSED[0m in 83.5s
//rllib:examples/custom_eval_torch                                       [0m[32mPASSED[0m in 81.3s
//rllib:examples/custom_fast_model_tf                                    [0m[32mPASSED[0m in 50.6s
//rllib:examples/custom_fast_model_torch                                 [0m[32mPASSED[0m in 36.4s
//rllib:examples/custom_keras_model_a2c                                  [0m[32mPASSED[0m in 160.9s
//rllib:examples/custom_keras_model_dqn                                  [0m[32mPASSED[0m in 50.3s
//rllib:examples/custom_keras_model_ppo                                  [0m[32mPASSED[0m in 51.5s
//rllib:examples/custom_loss_tf                                          [0m[32mPASSED[0m in 15.2s
//rllib:examples/custom_loss_torch                                       [0m[32mPASSED[0m in 23.6s
//rllib:examples/custom_metrics_and_callbacks                            [0m[32mPASSED[0m in 23.6s
//rllib:examples/custom_metrics_and_callbacks_legacy                     [0m[32mPASSED[0m in 24.0s
//rllib:examples/custom_model_api_tf                                     [0m[32mPASSED[0m in 7.6s
//rllib:examples/custom_model_api_torch                                  [0m[32mPASSED[0m in 4.7s
//rllib:examples/custom_observation_filters                              [0m[32mPASSED[0m in 23.5s
//rllib:examples/custom_rnn_model_repeat_after_me_tf                     [0m[32mPASSED[0m in 90.2s
//rllib:examples/custom_rnn_model_repeat_after_me_torch                  [0m[32mPASSED[0m in 74.0s
//rllib:examples/custom_rnn_model_repeat_initial_obs_tf                  [0m[32mPASSED[0m in 80.0s
//rllib:examples/custom_rnn_model_repeat_initial_obs_torch               [0m[32mPASSED[0m in 199.9s
//rllib:examples/custom_tf_policy                                        [0m[32mPASSED[0m in 39.2s
//rllib:examples/custom_torch_policy                                     [0m[32mPASSED[0m in 32.0s
//rllib:examples/cartpole_lstm_ppo_tf2                                  [0m[31m[1mTIMEOUT[0m in 3 out of 3 in 915.0s
  Stats over 3 runs: max = 915.0s, min = 915.0s, avg = 915.0s, dev = 0.0s
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/cartpole_lstm_ppo_tf2/test.log
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/cartpole_lstm_ppo_tf2/test_attempts/attempt_1.log
  /home/ray/.cache/bazel/_bazel_ray/f0514a01d094cb74312374fae7bac31d/execroot/com_github_ray_project_ray/bazel-out/k8-opt/testlogs/rllib/examples/cartpole_lstm_ppo_tf2/test_attempts/attempt_2.log

Executed 36 out of 36 tests: 35 tests pass and [0m[31m[1m1 fails locally[0m.
(06:25:31) [32mINFO: [0mBuild Event Protocol files produced successfully.
(06:25:31) [32mINFO:[0m Build completed, 1 test FAILED, 145 total actions
[0m(06:25:31) [32mINFO: [0mCurrent date is 2021-04-09
(06:25:31) [32mLoading:[0m 
(06:25:31) [32mLoading:[0m 0 packages loaded
(06:25:31) [33mDEBUG: [0m/home/ray/release-automation-rllib_unit_gpu_tests/ray/bazel/ray_deps_setup.bzl:63:14: No implicit mirrors used because urls were explicitly provided
(06:25:31) [32mAnalyzing:[0m 15 targets (0 packages loaded, 0 targets configured)
(06:25:31) [32mINFO: [0mAnalyzed 15 targets (0 packages loaded, 22 targets configured).
(06:25:31) [32mINFO: [0mFound 15 test targets...
(06:25:32) [32m[0 / 1][0m [Prepa] BazelWorkspaceStatusAction stable-status.txt
(06:25:51) [32m[7 / 9][0m Testing //rllib:examples/eager_execution; 19s local ... (2 actions running)
(06:26:15) [32m[14 / 16][0m 1 / 15 tests;[0m Testing //rllib:examples/multi_agent_two_trainers_torch; 43s local ... (2 actions running)
(06:26:31) [32m[18 / 20][0m 2 / 15 tests;[0m Testing //rllib:examples/parametric_actions_cartpole_pg_torch; 39s local ... (2 actions running)
(06:26:46) [32m[26 / 28][0m 4 / 15 tests;[0m Testing //rllib:examples/multi_agent_two_trainers_tf; 15s local ... (2 actions running)
(06:27:05) [32m[26 / 28][0m 4 / 15 tests;[0m Testing //rllib:examples/multi_agent_two_trainers_tf; 34s local ... (2 actions running)
(06:27:22) [32m[30 / 32][0m 5 / 15 tests;[0m Testing //rllib:examples/multi_agent_two_trainers_tf; 50s local ... (2 actions running)
(06:27:48) [32m[31 / 33][0m 6 / 15 tests;[0m Testing //rllib:examples/multi_agent_cartpole_torch; 42s local ... (2 actions running)
(06:28:08) [32m[35 / 37][0m 7 / 15 tests;[0m Testing //rllib:examples/hierarchical_training_tf; 46s local ... (2 actions running)
(06:28:56) [32m[35 / 37][0m 7 / 15 tests;[0m Testing //rllib:examples/hierarchical_training_tf; 93s local ... (2 actions running)
(06:29:57) [32m[39 / 41][0m 8 / 15 tests;[0m Testing //rllib:examples/nested_action_spaces_ppo_torch; 119s local ... (2 actions running)
(06:30:37) [32m[39 / 41][0m 8 / 15 tests;[0m Testing //rllib:examples/nested_action_spaces_ppo_torch; 159s local ... (2 actions running)
(06:31:23) [32m[51 / 53][0m 11 / 15 tests;[0m Testing //rllib:examples/hierarchical_training_torch; 18s local ... (2 actions running)
(06:32:15) [32m[51 / 53][0m 11 / 15 tests;[0m Testing //rllib:examples/hierarchical_training_torch; 71s local ... (2 actions running)
(06:33:16) [32m[51 / 53][0m 11 / 15 tests;[0m Testing //rllib:examples/hierarchical_training_torch; 132s local ... (2 actions running)
(06:34:27) [32m[55 / 57][0m 12 / 15 tests;[0m Testing //rllib:examples/hierarchical_training_torch; 203s local ... (2 actions running)
(06:35:07) [32mINFO: [0mElapsed time: 575.867s, Critical Path: 204.00s
(06:35:07) [32mINFO: [0m30 processes: 30 local.
(06:35:07) [32mINFO:[0m Build completed successfully, 61 total actions
//rllib:examples/eager_execution                                         [0m[32mPASSED[0m in 19.6s
//rllib:examples/hierarchical_training_tf                                [0m[32mPASSED[0m in 121.7s
//rllib:examples/hierarchical_training_torch                             [0m[32mPASSED[0m in 204.0s
//rllib:examples/multi_agent_cartpole_tf                                 [0m[32mPASSED[0m in 53.7s
//rllib:examples/multi_agent_cartpole_torch                              [0m[32mPASSED[0m in 52.6s
//rllib:examples/multi_agent_custom_policy_tf                            [0m[32mPASSED[0m in 27.3s
//rllib:examples/multi_agent_custom_policy_torch                         [0m[32mPASSED[0m in 22.7s
//rllib:examples/multi_agent_two_trainers_tf                             [0m[32mPASSED[0m in 50.9s
//rllib:examples/multi_agent_two_trainers_torch                          [0m[32mPASSED[0m in 43.3s
//rllib:examples/nested_action_spaces_ppo_tf                             [0m[32mPASSED[0m in 138.0s
//rllib:examples/nested_action_spaces_ppo_torch                          [0m[32mPASSED[0m in 186.1s
//rllib:examples/parametric_actions_cartpole_dqn_tf                      [0m[32mPASSED[0m in 39.5s
//rllib:examples/parametric_actions_cartpole_dqn_torch                   [0m[32mPASSED[0m in 78.6s
//rllib:examples/parametric_actions_cartpole_pg_tf                       [0m[32mPASSED[0m in 33.2s
//rllib:examples/parametric_actions_cartpole_pg_torch                    [0m[32mPASSED[0m in 39.5s

Executed 15 out of 15 tests: 15 tests pass.
(06:35:07) [32mINFO: [0mBuild Event Protocol files produced successfully.
(06:35:07) [32mINFO:[0m Build completed successfully, 61 total actions
(06:35:07) [32mINFO:[0m Build completed successfully, 61 total actions
[0m(06:35:07) [32mINFO: [0mCurrent date is 2021-04-09
(06:35:07) [32mLoading:[0m 
(06:35:07) [32mLoading:[0m 0 packages loaded
(06:35:07) [33mDEBUG: [0m/home/ray/release-automation-rllib_unit_gpu_tests/ray/bazel/ray_deps_setup.bzl:63:14: No implicit mirrors used because urls were explicitly provided
(06:35:07) [32mAnalyzing:[0m 14 targets (0 packages loaded, 0 targets configured)
(06:35:07) [32mINFO: [0mAnalyzed 14 targets (0 packages loaded, 20 targets configured).
(06:35:07) [32mINFO: [0mFound 14 test targets...
(06:35:07) [32m[0 / 4][0m [Prepa] BazelWorkspaceStatusAction stable-status.txt
(06:35:29) [32m[10 / 12][0m Testing //rllib:contrib/bandits/examples/lin_ts; 21s local ... (2 actions running)
(06:35:52) [32m[14 / 16][0m 1 / 14 tests;[0m Testing //rllib:examples/two_step_game_maddpg; 44s local ... (2 actions running)
(06:36:07) [32m[15 / 17][0m 2 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_torch; 37s local ... (2 actions running)
(06:36:22) [32m[15 / 17][0m 2 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_torch; 52s local ... (2 actions running)
(06:36:52) [32m[19 / 21][0m 3 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_torch; 82s local ... (2 actions running)
(06:37:24) [32m[19 / 21][0m 3 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_torch; 114s local ... (2 actions running)
(06:38:05) [32m[19 / 21][0m 3 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_torch; 155s local ... (2 actions running)
(06:38:55) [32m[23 / 25][0m 4 / 14 tests;[0m Testing //rllib:examples/rock_paper_scissors_multiagent_tf; 142s local ... (2 actions running)
(06:39:29) [32m[31 / 33][0m 6 / 14 tests;[0m Testing //rllib:examples/rollout_worker_custom_workflow; 34s local ... (2 actions running)
(06:40:11) [32m[39 / 41][0m 8 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_tf; 41s local ... (2 actions running)
(06:40:57) [32m[43 / 45][0m 9 / 14 tests;[0m Testing //rllib:examples/trajectory_view_api_tf; 87s local ... (2 actions running)
(06:41:50) [32m[55 / 57][0m 12 / 14 tests;[0m Testing //rllib:examples/rock_paper_scissors_multiagent_torch; 30s local ... (2 actions running)
(06:43:28) [32m[55 / 57][0m 12 / 14 tests;[0m Testing //rllib:examples/rock_paper_scissors_multiagent_torch; 128s local ... (2 actions running)
(06:44:03) [32mINFO: [0mElapsed time: 536.215s, Critical Path: 166.05s
(06:44:03) [32mINFO: [0m28 processes: 28 local.
(06:44:03) [32mINFO:[0m Build completed successfully, 57 total actions
//rllib:contrib/bandits/examples/lin_ts                                  [0m[32mPASSED[0m in 21.8s
//rllib:contrib/bandits/examples/lin_ucb                                 [0m[32mPASSED[0m in 28.4s
//rllib:examples/rock_paper_scissors_multiagent_tf                       [0m[32mPASSED[0m in 145.3s
//rllib:examples/rock_paper_scissors_multiagent_torch                    [0m[32mPASSED[0m in 156.2s
//rllib:examples/rollout_worker_custom_workflow                          [0m[32mPASSED[0m in 34.5s
//rllib:examples/trajectory_view_api_tf                                  [0m[32mPASSED[0m in 110.0s
//rllib:examples/trajectory_view_api_torch                               [0m[32mPASSED[0m in 166.0s
//rllib:examples/two_step_game_maddpg                                    [0m[32mPASSED[0m in 56.1s
//rllib:examples/two_step_game_pg_tf                                     [0m[32mPASSED[0m in 60.2s
//rllib:examples/two_step_game_pg_torch                                  [0m[32mPASSED[0m in 34.8s
//rllib:examples/two_step_game_qmix                                      [0m[32mPASSED[0m in 137.1s
//rllib:examples/two_trainer_workflow_mixed_torch_tf                     [0m[32mPASSED[0m in 39.5s
//rllib:examples/two_trainer_workflow_tf                                 [0m[32mPASSED[0m in 36.2s
//rllib:examples/two_trainer_workflow_torch                              [0m[32mPASSED[0m in 36.3s

Executed 14 out of 14 tests: 14 tests pass.
(06:44:03) [32mINFO: [0mBuild Event Protocol files produced successfully.
(06:44:03) [32mINFO:[0m Build completed successfully, 57 total actions
[0m~/release-automation-rllib_unit_gpu_tests

Script done on 2021-04-09 06:44:03-0700
